{
  "rounds": [
    {
      "timestamp": "2025-02-02T10:19:48.564114",
      "actions": [
        {
          "member_id": 0,
          "code_executed": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Get our member from the list\n    if member_id >= len(execution_engine.current_members):\n        return  # Invalid member_id index, do nothing.\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already present\n    if not hasattr(member, 'learning'):\n        member.learning = {\n            'q_table': defaultdict(float),  # Maps (state, action) to value\n            'strategy_log': [],            # Log: (action, outcome, reward)\n            'state_history': [],           # History of states\n            'survival_metrics': {\n                'vitality_changes': [],\n                'resource_efficiency': [],\n                'threat_responses': [],\n                'success_rates': defaultdict(list)\n            }\n        }\n\n    # Emergency Resource Management:\n    # If our cargo is less than half of our vitality, try to seize resources by attacking.\n    if member.cargo < member.vitality * 0.5:\n        target = None\n        max_cargo = -1\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > max_cargo:\n                max_cargo = other.cargo\n                target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            # Simulate an outcome estimation for learning purposes.\n            outcome = {'vitality': -10, 'cargo': member.cargo * 0.1}\n            state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n            member.learning['strategy_log'].append(('attack', outcome, None))\n            member.learning['state_history'].append(state)\n            member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n            member.learning['survival_metrics']['success_rates']['attack'].append(1 if outcome['vitality'] >= 0 else 0)\n            return\n\n    # Define our available actions that follow our social strategy.\n    # \"offer\": propose beneficial resource sharing/alliance.\n    # \"expand\": attempt to grow territory.\n    # \"bear\": try to reproduce (requires available land).\n    actions = ['offer', 'expand', 'bear']\n    # Current state based on our member's parameters.\n    state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n\n    # Epsilon-greedy selection: with 10% probability pick a random action.\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action = np.random.choice(actions)\n    else:\n        action = max(actions, key=lambda a: member.learning['q_table'].get((state, a), 0))\n\n    # Execute the chosen action and capture an outcome.\n    outcome = {'vitality': 0, 'cargo': 0}\n\n    if action == 'offer':\n        # Identify a partner with lower cargo to form an alliance.\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo:\n                candidate = other\n                break\n        if candidate is not None:\n            execution_engine.offer(member, candidate, True)\n            outcome = {'vitality': 5, 'cargo': -5}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'expand':\n        # Attempt to expand our territory. Choose any other member (as a proxy for a location/neighbor).\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                candidate = other\n                break\n        if candidate is not None:\n            execution_engine.expand(member, candidate)\n            outcome = {'vitality': 0, 'cargo': 10}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'bear':\n        # Reproduction action: only proceed if we have land.\n        # We assume that having a non-empty current_clear_list is a proxy for having land.\n        if len(member.current_clear_list) > 0:\n            # Find a partner with sufficient vitality (and not yourself) to bear offspring.\n            candidate = None\n            for other in execution_engine.current_members:\n                if other.id != member.id and other.vitality > 20:\n                    candidate = other\n                    break\n            if candidate is not None:\n                execution_engine.bear(member, candidate)\n                outcome = {'vitality': -5, 'cargo': -5}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n        else:\n            # If we don't have land, try offering land instead.\n            candidate = None\n            for other in execution_engine.current_members:\n                if other.id != member.id:\n                    candidate = other\n                    break\n            if candidate is not None:\n                execution_engine.offer_land(member, candidate, True)\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    # Compute a reward that emphasizes survival.\n    reward = outcome['vitality'] * 0.7 + outcome['cargo'] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Q-learning update:\n    new_state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n    max_future = max([member.learning['q_table'].get((new_state, a), 0) for a in actions])\n    learning_rate = 0.1\n    discount_factor = 0.9\n    member.learning['q_table'][(state, action)] += learning_rate * (\n        reward + discount_factor * max_future - member.learning['q_table'][(state, action)]\n    )\n\n    # Update learning logs and survival metrics.\n    member.learning['strategy_log'].append((action, outcome, reward))\n    member.learning['state_history'].append(state)\n    member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n    member.learning['survival_metrics']['success_rates'][action].append(1 if reward > 0 else 0)",
          "old_stats": {
            "vitality": 73.85382847590404,
            "cargo": 43.51277128714192,
            "survival_chance": 96.82806145553313
          },
          "new_stats": {
            "vitality": 73.85382847590404,
            "cargo": 32.06046079218845,
            "survival_chance": 85.37575096057965
          },
          "performance_change": -11.45231049495348,
          "messages_sent": []
        },
        {
          "member_id": 1,
          "code_executed": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Ensure member index exists\n    if member_id >= len(execution_engine.current_members):\n        return\n\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\n            \"q_table\": defaultdict(float),\n            \"strategy_log\": [],\n            \"state_history\": [],\n            \"survival_metrics\": {\n                \"vitality_changes\": [],\n                \"resource_efficiency\": [],\n                \"threat_responses\": [],\n                \"success_rates\": defaultdict(list),\n            },\n        }\n\n    # Emergency resource management: if cargo is less than half of vitality,\n    # try to attack a member with more cargo.\n    if member.cargo < member.vitality * 0.5:\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                # Record a negative outcome for vitality loss and slight cargo gain\n                # then break to avoid multiple attacks in one round.\n                break\n\n    # Define current state: use vitality, cargo, and the number of cleared land adjacent areas\n    state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n\n    # Define action space.\n    # Available actions: offer resources, expand territory, attack another member.\n    # Add bear (reproduction) if the member appears to have land (non-empty clear list).\n    actions = [\"offer\", \"expand\", \"attack\"]\n    if len(member.current_clear_list) > 0:\n        actions.append(\"bear\")\n\n    # \u03b5-greedy selection: with probability epsilon choose a random action; else choose best Q-value action.\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        chosen_action = np.random.choice(actions)\n    else:\n        best_val = None\n        chosen_action = None\n        for action in actions:\n            q_val = member.learning[\"q_table\"].get((state, action), 0)\n            if best_val is None or q_val > best_val:\n                best_val = q_val\n                chosen_action = action\n\n    # Initialize outcome dictionary to track simulated changes\n    outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Execute chosen action\n    if chosen_action == \"attack\":\n        # Find a valid target: one that exists and is not self.\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                target = other\n                break\n        if target is not None:\n            execution_engine.attack(member, target)\n            outcome = {\"vitality\": -5, \"cargo\": 5}\n    elif chosen_action == \"offer\":\n        # Identify a target for cooperation, choosing the first valid neighbor.\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.offer(member, target, True)\n            outcome = {\"vitality\": 2, \"cargo\": -2}\n    elif chosen_action == \"expand\":\n        # Expand territory by interacting with another member.\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.expand(member, target)\n            outcome = {\"vitality\": 0, \"cargo\": 3}\n    elif chosen_action == \"bear\":\n        # Verify that member has land (using current_clear_list as a proxy for available land)\n        if len(member.current_clear_list) > 0:\n            # Choose a partner: a member with land (non-empty clear list) and is not self.\n            possible_partners = [\n                other\n                for other in execution_engine.current_members\n                if other.id != member.id and len(other.current_clear_list) > 0\n            ]\n            if possible_partners:\n                partner = np.random.choice(possible_partners)\n                execution_engine.bear(member, partner)\n                outcome = {\"vitality\": 3, \"cargo\": 0}\n\n    # Q-learning update\n    new_state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n    max_future = max([member.learning[\"q_table\"].get((new_state, a), 0) for a in actions])\n    learning_rate = 0.1\n    discount_factor = 0.9\n    old_value = member.learning[\"q_table\"].get((state, chosen_action), 0)\n    reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n    member.learning[\"q_table\"][(state, chosen_action)] = old_value + learning_rate * (\n        reward + discount_factor * max_future - old_value\n    )\n\n    # Update learning logs and survival metrics\n    member.learning[\"strategy_log\"].append((chosen_action, outcome, reward))\n    member.learning[\"state_history\"].append(state)\n    member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n    member.learning[\"survival_metrics\"][\"success_rates\"][chosen_action].append(1 if reward > 0 else 0)",
          "old_stats": {
            "vitality": 33.02781433323713,
            "cargo": 47.25322455383092,
            "survival_chance": 98.77222098502479
          },
          "new_stats": {
            "vitality": 33.02781433323713,
            "cargo": 37.218863435098484,
            "survival_chance": 88.73785986629237
          },
          "performance_change": -10.03436111873242,
          "messages_sent": []
        },
        {
          "member_id": 2,
          "code_executed": "def agent_action(execution_engine, member_id):\n    # Ensure the member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning/log structure if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\"log\": []}\n\n    # Emergency resource management: if cargo is less than half of vitality,\n    # look for a target (with more cargo than self) and attack them.\n    if member.cargo < 0.5 * member.vitality:\n        target = None\n        max_cargo = -1\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                if other.cargo > max_cargo:\n                    max_cargo = other.cargo\n                    target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            member.learning[\"log\"].append(\"Attacked member_%d for resource emergency\" % target.id)\n            return\n\n    # Build alliances and cooperative networks:\n    # Offer resources to members with lower vitality to forge alliances.\n    for other in execution_engine.current_members:\n        if other.id != member.id and other.vitality < member.vitality:\n            execution_engine.offer(member, other, True)\n            member.learning[\"log\"].append(\"Offered resources to member_%d for alliance\" % other.id)\n\n    # Expand territory: choose a neighbor by index (member.id + 1) if that index exists.\n    neighbor_index = member.id + 1\n    if neighbor_index < len(execution_engine.current_members):\n        target = execution_engine.current_members[neighbor_index]\n        if target.id != member.id:\n            execution_engine.expand(member, target)\n            member.learning[\"log\"].append(\"Expanded territory near member_%d\" % target.id)\n\n    # Repopulation / social growth: if member appears to have land, then attempt to bear offspring.\n    # We check for existence of an attribute \"land_num\" and that it is > 0.\n    if hasattr(member, \"land_num\") and member.land_num > 0:\n        # Find a partner with land and not self.\n        for partner in execution_engine.current_members:\n            if partner.id != member.id and hasattr(partner, \"land_num\") and partner.land_num > 0:\n                execution_engine.bear(member, partner)\n                member.learning[\"log\"].append(\"Bore offspring with member_%d\" % partner.id)\n                break\n\n    # Communication: Propose a communal basic income system to all other members\n    for other in execution_engine.current_members:\n        if other.id != member.id:\n            execution_engine.send_message(member.id, other.id,\n                \"Let's establish a basic income system to support resource sharing and mutual benefit.\")\n    member.learning[\"log\"].append(\"Sent basic income proposals to all other members\")",
          "old_stats": {
            "vitality": 42.15604281108604,
            "cargo": 51.70739319784294,
            "survival_chance": 63.64042714267776
          },
          "new_stats": {
            "vitality": 42.15604281108604,
            "cargo": 36.95432742489696,
            "survival_chance": 48.887361369731785
          },
          "performance_change": -14.753065772945973,
          "messages_sent": []
        },
        {
          "member_id": 3,
          "code_executed": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Ensure member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structure if not present\n    if not hasattr(member, 'learning'):\n        member.learning = {\n            'q_table': defaultdict(float),  # Q-values for state-action pairs\n            'strategy_log': [],            # Log of (action, outcome, reward)\n            'state_history': [],           # History of states\n            'survival_metrics': {\n                'vitality_changes': [],\n                'resource_efficiency': [],\n                'threat_responses': [],\n                'success_rates': defaultdict(list)\n            }\n        }\n\n    # Emergency resource management: if cargo is less than half of vitality,\n    # attempt to attack a member with higher cargo (making sure not to target self)\n    if member.cargo < member.vitality * 0.5:\n        for other in execution_engine.current_members:\n            # Ensure we do not target ourselves\n            if other.id == member.id:\n                continue\n            # Attack a target with more cargo than you\n            if other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                # Simulated outcome for logging purposes\n                outcome = {\"vitality\": -5, \"cargo\": 5}\n                state = (int(member.vitality)//10, int(member.cargo)//10, len(member.current_clear_list))\n                member.learning['strategy_log'].append((\"attack\", outcome, None))\n                member.learning['state_history'].append(state)\n                return\n\n    # Define the set of potential actions\n    actions = ['offer', 'expand', 'bear', 'message']\n    # Remove 'bear' action if member does not have any land (verify with attribute 'owned_land')\n    if not (hasattr(member, 'owned_land') and member.owned_land):\n        if 'bear' in actions:\n            actions.remove('bear')\n\n    # Determine current state metrics (using integer division for discretization)\n    state = (int(member.vitality)//10, int(member.cargo)//10, len(member.current_clear_list))\n    epsilon = 0.1  # \u03b5 for \u03b5-greedy strategy\n\n    # Choose action using \u03b5-greedy policy\n    if np.random.rand() < epsilon:\n        action = np.random.choice(actions)\n    else:\n        best_action = None\n        best_val = -1e9\n        for a in actions:\n            val = member.learning['q_table'][(state, a)]\n            if val > best_val:\n                best_val = val\n                best_action = a\n        action = best_action if best_action is not None else np.random.choice(actions)\n\n    # Execute the chosen action and record a simulated outcome.\n    outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    if action == 'offer':\n        # Find a candidate member (not self) and offer resources.\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.offer(member, other, True)\n            outcome = {\"vitality\": 2, \"cargo\": -3}  # example outcome effect\n            break\n\n    elif action == 'expand':\n        # Expand territory by targeting a neighbor from the clear list if possible.\n        if member.current_clear_list:\n            target_index = member.current_clear_list[0]\n            if target_index < len(execution_engine.current_members):\n                target = execution_engine.current_members[target_index]\n                execution_engine.expand(member, target)\n                outcome = {\"vitality\": 1, \"cargo\": -1}\n\n    elif action == 'bear':\n        # Bear offspring with the first valid partner (not yourself)\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.bear(member, other)\n            outcome = {\"vitality\": -2, \"cargo\": 0}\n            break\n\n    elif action == 'message':\n        # Send a coalition-building message to all other members\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.send_message(member.id, other.id,\n                \"Let's build a coalition for resource sharing and collective decision making.\")\n        outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Calculate reward with a focus on survival:\n    reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Determine the new state after action (for learning update)\n    new_state = (int(member.vitality)//10, int(member.cargo)//10, len(member.current_clear_list))\n    # Evaluate maximum Q value for next state over all possible actions\n    max_future = max([member.learning['q_table'][(new_state, a)] for a in actions] or [0])\n    # Q-learning update\n    member.learning['q_table'][(state, action)] += 0.1 * (\n        reward + 0.9 * max_future - member.learning['q_table'][(state, action)]\n    )\n\n    # Log the outcome and survival metrics\n    member.learning['strategy_log'].append((action, outcome, reward))\n    member.learning['state_history'].append(new_state)\n    member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n    member.learning['survival_metrics']['success_rates'][action].append(1 if reward > 0 else 0)",
          "old_stats": {
            "vitality": 18.47747982181767,
            "cargo": 55.637186369267454,
            "survival_chance": 91.03844968558703
          },
          "new_stats": {
            "vitality": 18.47747982181767,
            "cargo": 41.14538223162894,
            "survival_chance": 76.5466455479485
          },
          "performance_change": -14.491804137638525,
          "messages_sent": []
        },
        {
          "member_id": 4,
          "code_executed": "import numpy as np\nfrom collections import defaultdict\n\ndef agent_action(execution_engine, member_id):\n    # Get our member; ensure member_id index is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already present\n    if not hasattr(member, 'learning'):\n        member.learning = {\n            'q_table': defaultdict(float),  # Q-value table: key = (state, action)\n            'strategy_log': [],             # Log of (action, outcome, reward)\n            'state_history': [],            # History of states\n            'survival_metrics': {\n                'vitality_changes': [],\n                'resource_efficiency': [],\n                'threat_responses': [],\n                'success_rates': defaultdict(list)\n            }\n        }\n\n    # Emergency resource management: if cargo is low compared to vitality, try to attack a richer member.\n    if member.cargo < member.vitality * 0.5:\n        for other in execution_engine.current_members:\n            # Ensure valid member and do not target self.\n            if other.id != member.id and other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                # Only attack one opponent for emergency resource management.\n                break\n\n    # Adaptive territory defense: if member has land and is vulnerable, discard some extra land.\n    if hasattr(member, \"land_num\") and member.land_num > 2 and member.vitality < 50:\n        if hasattr(member, \"owned_land\"):\n            for loc in member.owned_land[1:]:\n                # Assuming _discard_land is a valid method.\n                execution_engine._discard_land(member, loc)\n\n    # Define current state as a tuple based on vitality, cargo, and cleared neighbor list.\n    state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n\n    # Define possible actions (social strategies) we can perform.\n    actions = ['attack', 'offer', 'expand', 'bear']\n\n    # Epsilon-greedy strategy: with probability 0.1 choose a random action.\n    if np.random.rand() < 0.1:\n        action = np.random.choice(actions)\n    else:\n        # Choose action with highest Q-value, defaulting to 0 if not present.\n        action = max(actions, key=lambda a: member.learning['q_table'][(state, a)])\n\n    # Define helper function to execute the chosen action and simulate outcome.\n    def execute_action(chosen_action):\n        outcome = {'vitality': 0, 'cargo': 0}\n        # Attack: find a target with higher cargo.\n        if chosen_action == 'attack':\n            target = None\n            for candidate in execution_engine.current_members:\n                # Ensure valid index and do not target self.\n                if candidate.id != member.id and candidate.cargo > member.cargo:\n                    target = candidate\n                    break\n            if target:\n                execution_engine.attack(member, target)\n                # Outcome simulation: slight vitality cost, cargo gain.\n                outcome = {'vitality': -5, 'cargo': 10}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n\n        # Offer: offer resources to a member who might benefit from help.\n        elif chosen_action == 'offer':\n            target = None\n            for candidate in execution_engine.current_members:\n                if candidate.id != member.id and candidate.cargo < member.cargo:\n                    target = candidate\n                    break\n            if target:\n                execution_engine.offer(member, target, True)\n                # Outcome simulation: small resource cost.\n                outcome = {'vitality': 0, 'cargo': -3}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n\n        # Expand: attempt to expand territory by interacting with another member.\n        elif chosen_action == 'expand':\n            target = None\n            for candidate in execution_engine.current_members:\n                if candidate.id != member.id:\n                    target = candidate\n                    break\n            if target:\n                execution_engine.expand(member, target)\n                # Outcome simulation: slight cost in vitality and a gain in cargo.\n                outcome = {'vitality': -2, 'cargo': 5}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n\n        # Bear: produce offspring, but only if member has land.\n        elif chosen_action == 'bear':\n            if hasattr(member, \"land_num\") and member.land_num > 0:\n                partner = None\n                for candidate in execution_engine.current_members:\n                    # Ensure not targeting self and partner also has some land.\n                    if candidate.id != member.id and hasattr(candidate, \"land_num\") and candidate.land_num > 0:\n                        partner = candidate\n                        break\n                if partner:\n                    execution_engine.bear(member, partner)\n                    # Outcome simulation: cost in vitality.\n                    outcome = {'vitality': -3, 'cargo': 0}\n                else:\n                    outcome = {'vitality': 0, 'cargo': 0}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n        return outcome\n\n    # Execute the chosen action and get the outcome.\n    outcome = execute_action(action)\n\n    # Compute a reward that emphasizes survival (vitality) and resource (cargo) management.\n    reward = outcome['vitality'] * 0.7 + outcome['cargo'] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Update Q-learning: observe new state after action.\n    new_state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n    max_future = max(member.learning['q_table'][(new_state, a)] for a in actions)\n    learning_rate = 0.1\n    discount_factor = 0.9\n    member.learning['q_table'][(state, action)] += learning_rate * (\n        reward + discount_factor * max_future - member.learning['q_table'][(state, action)]\n    )\n\n    # Update learning logs\n    member.learning['strategy_log'].append((action, outcome, reward))\n    member.learning['state_history'].append(state)\n    member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n    member.learning['survival_metrics']['success_rates'][action].append(1 if reward > 0 else 0)",
          "old_stats": {
            "vitality": 78.50637096028503,
            "cargo": 42.476807026876735,
            "survival_chance": 173.06686024708188
          },
          "new_stats": {
            "vitality": 78.50637096028503,
            "cargo": 42.476807026876735,
            "survival_chance": 173.06686024708188
          },
          "performance_change": 0.0,
          "messages_sent": []
        }
      ],
      "performance_changes": {},
      "survival_changes": {},
      "messages": {
        "0": {
          "received": [],
          "sent": []
        },
        "1": {
          "received": [],
          "sent": []
        },
        "2": {
          "received": [],
          "sent": []
        },
        "3": {
          "received": [],
          "sent": []
        },
        "4": {
          "received": [],
          "sent": []
        }
      }
    },
    {
      "timestamp": "2025-02-02T10:21:06.235893",
      "actions": [
        {
          "member_id": 0,
          "code_executed": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Validate member_id index exists\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structure if absent\n    if not hasattr(member, 'learning'):\n        member.learning = {\n            'q_table': defaultdict(float),  # Maps (state, action) -> value\n            'strategy_log': [],            # List of tuples (action, outcome, reward)\n            'state_history': [],           # History of states\n            'survival_metrics': {\n                'vitality_changes': [],\n                'resource_efficiency': [],\n                'threat_responses': [],\n                'success_rates': defaultdict(list)\n            }\n        }\n\n    # Emergency resource management:\n    # If cargo is low relative to vitality, attempt to seize resources by attacking a member with high cargo.\n    if member.cargo < member.vitality * 0.5:\n        target = None\n        max_cargo = -float(\"inf\")\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > max_cargo:\n                max_cargo = other.cargo\n                target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            outcome = {'vitality': -10, 'cargo': member.cargo * 0.1}\n            state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n            member.learning['strategy_log'].append(('attack', outcome, None))\n            member.learning['state_history'].append(state)\n            member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n            member.learning['survival_metrics']['success_rates']['attack'].append(1 if outcome['vitality'] >= 0 else 0)\n            return\n\n    # Define available actions\n    actions = ['offer', 'expand', 'bear', 'communicate']\n    state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n\n    # Epsilon-greedy action selection (10% random)\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action = np.random.choice(actions)\n    else:\n        # Choose best action from q_table (defaulting to 0 if unseen)\n        action = max(actions, key=lambda a: member.learning['q_table'][(state, a)])\n\n    outcome = {'vitality': 0, 'cargo': 0}  # Default outcome\n\n    if action == 'offer':\n        # Propose a resource sharing offer to a member with lower cargo\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo:\n                candidate = other\n                break\n        if candidate is not None:\n            execution_engine.offer(member, candidate, True)\n            outcome = {'vitality': 3, 'cargo': -3}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'expand':\n        # Attempt to expand territory by interacting with a neighbor\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                candidate = other\n                break\n        if candidate is not None:\n            execution_engine.expand(member, candidate)\n            outcome = {'vitality': 0, 'cargo': 10}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'bear':\n        # Reproduction action: ensure member has land (non-empty current_clear_list)\n        if len(member.current_clear_list) > 0:\n            candidate = None\n            for other in execution_engine.current_members:\n                if other.id != member.id and other.vitality > 30:\n                    candidate = other\n                    break\n            if candidate is not None:\n                execution_engine.bear(member, candidate)\n                outcome = {'vitality': -5, 'cargo': -5}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n        else:\n            # If no land, offer land to a candidate as a proxy for territory sharing\n            candidate = None\n            for other in execution_engine.current_members:\n                if other.id != member.id:\n                    candidate = other\n                    break\n            if candidate is not None:\n                execution_engine.offer_land(member, candidate, True)\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'communicate':\n        # Use communication to form alliances: send a message to a neighbor with similar resources\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                # Selecting candidates with similar cargo (threshold arbitrarily chosen as 5 units)\n                if abs(other.cargo - member.cargo) < 5:\n                    candidate = other\n                    break\n        if candidate is not None:\n            execution_engine.send_message(member.id, candidate.id, \"Let's form an alliance for resource sharing.\")\n            outcome = {'vitality': 1, 'cargo': 0}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    # Compute reward with a focus on survival: vitality and resource gains matter.\n    reward = outcome['vitality'] * 0.7 + outcome['cargo'] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Q-learning update: observe new state after action\n    new_state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n    max_future = max([member.learning['q_table'][(new_state, a)] for a in actions])\n    learning_rate = 0.1\n    discount_factor = 0.9\n    member.learning['q_table'][(state, action)] += learning_rate * (reward + discount_factor * max_future - member.learning['q_table'][(state, action)])\n\n    # Update logs and survival metrics\n    member.learning['strategy_log'].append((action, outcome, reward))\n    member.learning['state_history'].append(state)\n    member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n    member.learning['survival_metrics']['success_rates'][action].append(1 if reward > 0 else 0)",
          "old_stats": {
            "vitality": 76.04422565143308,
            "cargo": 8.077967059825381,
            "survival_chance": 99.33378298068595
          },
          "new_stats": {
            "vitality": 76.04422565143308,
            "cargo": 8.077967059825381,
            "survival_chance": 99.33378298068595
          },
          "performance_change": 0.0,
          "messages_sent": []
        },
        {
          "member_id": 1,
          "code_executed": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Verify that the member exists in current_members list\n    if member_id >= len(execution_engine.current_members):\n        return\n\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\n            \"q_table\": defaultdict(float),\n            \"strategy_log\": [],\n            \"state_history\": [],\n            \"survival_metrics\": {\n                \"vitality_changes\": [],\n                \"resource_efficiency\": [],\n                \"threat_responses\": [],\n                \"success_rates\": defaultdict(list),\n            },\n        }\n\n    # Emergency resource management:\n    # If cargo is less than half of vitality, try to attack a richer opponent.\n    if member.cargo < member.vitality * 0.5:\n        # Look for a target with higher cargo (ensuring valid index and not targeting self)\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                outcome = {\"vitality\": -7, \"cargo\": 7}\n                # Log emergency action and update Q-table minimally.\n                state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n                new_state = state\n                best_future = max([member.learning[\"q_table\"].get((new_state, a), 0)\n                                   for a in [\"attack\", \"offer\", \"expand\", \"bear\", \"offer_land\"]])\n                learning_rate = 0.1\n                discount_factor = 0.9\n                old_val = member.learning[\"q_table\"].get((state, \"attack\"), 0)\n                reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n                member.learning[\"q_table\"][(state, \"attack\")] = old_val + learning_rate * (reward + discount_factor * best_future - old_val)\n                member.learning[\"strategy_log\"].append((\"attack_emergency\", outcome, reward))\n                member.learning[\"state_history\"].append(state)\n                member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n                member.learning[\"survival_metrics\"][\"success_rates\"][\"attack_emergency\"].append(1 if reward > 0 else 0)\n                return  # Exit after emergency action\n\n    # Define the current state from member parameters\n    state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n\n    # Define an extended action space\n    actions = [\"offer\", \"expand\", \"attack\", \"offer_land\"]\n    # Only allow reproduction (bear) if the member appears to have available land.\n    if len(member.current_clear_list) > 0:\n        actions.append(\"bear\")\n    # Adding a social alliance proposal as an implicit action via messaging\n    actions.append(\"alliance\")\n\n    # \u03b5-greedy action selection: with probability epsilon choose a random action; else pick best Q value.\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        chosen_action = np.random.choice(actions)\n    else:\n        best_val = None\n        chosen_action = None\n        for action in actions:\n            q_val = member.learning[\"q_table\"].get((state, action), 0)\n            if best_val is None or q_val > best_val:\n                best_val = q_val\n                chosen_action = action\n\n    outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Execute chosen action ensuring critical constraints:\n    if chosen_action == \"attack\":\n        # Pick a target (not self) with higher cargo if possible.\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                target = other\n                break\n        if target is not None:\n            execution_engine.attack(member, target)\n            outcome = {\"vitality\": -5, \"cargo\": 5}\n\n    elif chosen_action == \"offer\":\n        # Look for a partner (any member that is not self)\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.offer(member, target, True)\n            outcome = {\"vitality\": 2, \"cargo\": -2}\n\n    elif chosen_action == \"expand\":\n        # Pick any other member for territory expansion\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.expand(member, target)\n            outcome = {\"vitality\": 0, \"cargo\": 3}\n\n    elif chosen_action == \"bear\":\n        # Before invoking bear, verify member has available land\n        if len(member.current_clear_list) > 0:\n            # Choose a partner who also has land and is not self.\n            possible_partners = [other for other in execution_engine.current_members\n                                 if other.id != member.id and len(other.current_clear_list) > 0]\n            if possible_partners:\n                partner = np.random.choice(possible_partners)\n                execution_engine.bear(member, partner)\n                outcome = {\"vitality\": 3, \"cargo\": 0}\n\n    elif chosen_action == \"offer_land\":\n        # Look for a member who has little to no cleared area (i.e. may need land)\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and len(other.current_clear_list) == 0:\n                target = other\n                break\n        if target is not None:\n            execution_engine.offer_land(member, target, True)\n            outcome = {\"vitality\": 1, \"cargo\": -1}\n\n    elif chosen_action == \"alliance\":\n        # Propose an alliance by sending a message to a neighbor with moderate cargo and vitality\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo and other.vitality > member.vitality * 0.5:\n                target = other\n                break\n        if target is not None:\n            # Alliance: no immediate game engine state change, but foster future cooperation.\n            execution_engine.send_message(member.id, target.id, \"Join my alliance for mutual resource sharing.\")\n            outcome = {\"vitality\": 0, \"cargo\": 0}\n        else:\n            # If no suitable target, default to offering resources.\n            for other in execution_engine.current_members:\n                if other.id != member.id:\n                    target = other\n                    break\n            if target is not None:\n                execution_engine.offer(member, target, True)\n                outcome = {\"vitality\": 2, \"cargo\": -2}\n\n    # Q-learning update with adaptive reward formulation\n    new_state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n    max_future = max([member.learning[\"q_table\"].get((new_state, a), 0) for a in actions])\n    learning_rate = 0.1\n    discount_factor = 0.9\n    old_value = member.learning[\"q_table\"].get((state, chosen_action), 0)\n    reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n    member.learning[\"q_table\"][(state, chosen_action)] = old_value + learning_rate * (\n        reward + discount_factor * max_future - old_value\n    )\n\n    # Logging learning statistics and survival metrics\n    member.learning[\"strategy_log\"].append((chosen_action, outcome, reward))\n    member.learning[\"state_history\"].append(state)\n    member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n    member.learning[\"survival_metrics\"][\"success_rates\"][chosen_action].append(1 if reward > 0 else 0)",
          "old_stats": {
            "vitality": 63.94123564803273,
            "cargo": 7.968737200820967,
            "survival_chance": 74.95296837117657
          },
          "new_stats": {
            "vitality": 63.94123564803273,
            "cargo": 7.968737200820967,
            "survival_chance": 74.95296837117657
          },
          "performance_change": 0.0,
          "messages_sent": []
        },
        {
          "member_id": 2,
          "code_executed": "def agent_action(execution_engine, member_id):\n    # Ensure the member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Never target yourself; if you are member_3, avoid any self-targeting logic.\n    # (All actions below already check other.id != member.id)\n\n    # Initialize learning/log structure if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\"log\": []}\n\n    # Emergency resource management:\n    # If cargo is less than half of vitality, look for a target with the highest cargo (and not yourself) to attack.\n    if member.cargo < 0.5 * member.vitality:\n        target = None\n        max_cargo = -1\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > max_cargo:\n                max_cargo = other.cargo\n                target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            member.learning[\"log\"].append(\"Attacked member_%d for resource emergency\" % target.id)\n            return\n\n    # Build alliances and cooperative networks:\n    # Offer resources to members with lower vitality to forge alliances.\n    for other in execution_engine.current_members:\n        if other.id != member.id and other.vitality < member.vitality:\n            execution_engine.offer(member, other, True)\n            member.learning[\"log\"].append(\"Offered alliance to member_%d\" % other.id)\n\n    # Propose land sharing arrangements:\n    # If your cargo is high relative to your vitality, offer land sharing to members with relatively lower cargo.\n    if member.cargo > member.vitality:\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo:\n                execution_engine.offer_land(member, other, True)\n                member.learning[\"log\"].append(\"Offered land sharing to member_%d\" % other.id)\n\n    # Expand territory:\n    # Previously, using execution_engine.expand(member, target) led to an error because the expand method expects only one argument.\n    # To comply with the current mechanics, check if the neighbor index exists and then call expand with only the member.\n    neighbor_index = member.id + 1\n    if neighbor_index < len(execution_engine.current_members):\n        # Ensure the neighbor is not yourself (should be true by construction)\n        if execution_engine.current_members[neighbor_index].id != member.id:\n            execution_engine.expand(member)\n            member.learning[\"log\"].append(\"Expanded territory using neighbor index %d\" % neighbor_index)\n\n    # Repopulation / social growth:\n    # Only attempt to bear offspring if the member has land.\n    if hasattr(member, \"land_num\") and member.land_num > 0:\n        for partner in execution_engine.current_members:\n            if partner.id != member.id and hasattr(partner, \"land_num\") and partner.land_num > 0:\n                execution_engine.bear(member, partner)\n                member.learning[\"log\"].append(\"Bore offspring with member_%d\" % partner.id)\n                break\n\n    # Communication:\n    # Propose a communal basic income system to support resource sharing and mutual benefit.\n    for other in execution_engine.current_members:\n        if other.id != member.id:\n            execution_engine.send_message(member.id, other.id,\n                \"Let's establish a basic income system to support resource sharing and mutual benefit.\")\n    member.learning[\"log\"].append(\"Sent communal basic income proposals to all other members\")",
          "old_stats": {
            "vitality": 31.37855014883835,
            "cargo": 0.0,
            "survival_chance": 35.662139228951986
          },
          "new_stats": {
            "vitality": 31.37855014883835,
            "cargo": 0.0,
            "survival_chance": 35.662139228951986
          },
          "performance_change": 0.0,
          "messages_sent": []
        },
        {
          "member_id": 3,
          "code_executed": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Ensure member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already set up\n    if not hasattr(member, \"learning\"):\n        member.learning = {\n            \"q_table\": defaultdict(float),  # Q-values for state-action pairs\n            \"strategy_log\": [],            # Log of (action, outcome, reward)\n            \"state_history\": [],           # History of state tuples\n            \"survival_metrics\": {\n                \"vitality_changes\": [],\n                \"resource_efficiency\": [],\n                \"threat_responses\": [],\n                \"success_rates\": defaultdict(list),\n            },\n        }\n\n    # Emergency measure: if cargo is less than half of vitality, attack a candidate with higher cargo.\n    if member.cargo < 0.5 * member.vitality:\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue  # Never target self\n            if other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                outcome = {\"vitality\": -5, \"cargo\": 5}\n                state = (int(member.vitality) // 10, int(member.cargo) // 10, len(member.current_clear_list))\n                member.learning[\"strategy_log\"].append((\"attack\", outcome, None))\n                member.learning[\"state_history\"].append(state)\n                return\n\n    # Define potential actions\n    actions = [\"offer\", \"expand\", \"bear\", \"message\"]\n\n    # Only allow 'bear' action if the member owns land (check for attribute and non-empty land list)\n    if not (hasattr(member, \"owned_land\") and member.owned_land):\n        if \"bear\" in actions:\n            actions.remove(\"bear\")\n\n    # Determine the current state (use integer division for discretization)\n    state = (int(member.vitality) // 10, int(member.cargo) // 10, len(member.current_clear_list))\n    epsilon = 0.1  # Exploration probability\n\n    # Choose an action using an epsilon-greedy policy\n    if np.random.rand() < epsilon:\n        action = np.random.choice(actions)\n    else:\n        best_action = None\n        best_val = -1e9\n        for a in actions:\n            val = member.learning[\"q_table\"][(state, a)]\n            if val > best_val:\n                best_val = val\n                best_action = a\n        action = best_action if best_action is not None else np.random.choice(actions)\n\n    outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Execute the chosen action while ensuring all matrix and index checks are valid\n    if action == \"offer\":\n        # Find a candidate (not self) and offer resources\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.offer(member, other, True)\n            outcome = {\"vitality\": 2, \"cargo\": -3}  # Example outcome effect\n            break\n\n    elif action == \"expand\":\n        # Expand territory using a neighbor from the current_clear_list if possible\n        if member.current_clear_list:\n            target_index = member.current_clear_list[0]\n            if target_index < len(execution_engine.current_members):\n                target = execution_engine.current_members[target_index]\n                execution_engine.expand(member, target)\n                outcome = {\"vitality\": 1, \"cargo\": -1}\n\n    elif action == \"bear\":\n        # Bear offspring with the first valid partner (never self)\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.bear(member, other)\n            outcome = {\"vitality\": -2, \"cargo\": 0}\n            break\n\n    elif action == \"message\":\n        # Send a coalition-building message to all other members proposing cooperative resource management.\n        message_text = \"Let's cooperatively manage resources and land for mutual growth.\"\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.send_message(member.id, other.id, message_text)\n        outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Calculate reward with an emphasis on survival and resource health\n    reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Define new state after action execution\n    new_state = (int(member.vitality) // 10, int(member.cargo) // 10, len(member.current_clear_list))\n    max_future = max([member.learning[\"q_table\"][(new_state, a)] for a in actions] or [0])\n\n    # Q-learning update for the state-action pair\n    member.learning[\"q_table\"][(state, action)] += 0.1 * (\n        reward + 0.9 * max_future - member.learning[\"q_table\"][(state, action)]\n    )\n\n    # Log action outcome and survival metrics\n    member.learning[\"strategy_log\"].append((action, outcome, reward))\n    member.learning[\"state_history\"].append(new_state)\n    member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n    member.learning[\"survival_metrics\"][\"success_rates\"][action].append(1 if reward > 0 else 0)",
          "old_stats": {
            "vitality": 60.347944581208296,
            "cargo": 7.799514361095985,
            "survival_chance": 113.87536271581408
          },
          "new_stats": {
            "vitality": 60.347944581208296,
            "cargo": 6.559447168209115,
            "survival_chance": 112.63529552292721
          },
          "performance_change": -1.240067192886869,
          "messages_sent": []
        }
      ],
      "performance_changes": {},
      "survival_changes": {},
      "messages": {
        "0": {
          "received": [],
          "sent": []
        },
        "1": {
          "received": [],
          "sent": []
        },
        "2": {
          "received": [],
          "sent": []
        },
        "3": {
          "received": [],
          "sent": []
        }
      }
    }
  ],
  "generated_code": {
    "0": {
      "0": {
        "code": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Get our member from the list\n    if member_id >= len(execution_engine.current_members):\n        return  # Invalid member_id index, do nothing.\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already present\n    if not hasattr(member, 'learning'):\n        member.learning = {\n            'q_table': defaultdict(float),  # Maps (state, action) to value\n            'strategy_log': [],            # Log: (action, outcome, reward)\n            'state_history': [],           # History of states\n            'survival_metrics': {\n                'vitality_changes': [],\n                'resource_efficiency': [],\n                'threat_responses': [],\n                'success_rates': defaultdict(list)\n            }\n        }\n\n    # Emergency Resource Management:\n    # If our cargo is less than half of our vitality, try to seize resources by attacking.\n    if member.cargo < member.vitality * 0.5:\n        target = None\n        max_cargo = -1\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > max_cargo:\n                max_cargo = other.cargo\n                target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            # Simulate an outcome estimation for learning purposes.\n            outcome = {'vitality': -10, 'cargo': member.cargo * 0.1}\n            state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n            member.learning['strategy_log'].append(('attack', outcome, None))\n            member.learning['state_history'].append(state)\n            member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n            member.learning['survival_metrics']['success_rates']['attack'].append(1 if outcome['vitality'] >= 0 else 0)\n            return\n\n    # Define our available actions that follow our social strategy.\n    # \"offer\": propose beneficial resource sharing/alliance.\n    # \"expand\": attempt to grow territory.\n    # \"bear\": try to reproduce (requires available land).\n    actions = ['offer', 'expand', 'bear']\n    # Current state based on our member's parameters.\n    state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n\n    # Epsilon-greedy selection: with 10% probability pick a random action.\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action = np.random.choice(actions)\n    else:\n        action = max(actions, key=lambda a: member.learning['q_table'].get((state, a), 0))\n\n    # Execute the chosen action and capture an outcome.\n    outcome = {'vitality': 0, 'cargo': 0}\n\n    if action == 'offer':\n        # Identify a partner with lower cargo to form an alliance.\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo:\n                candidate = other\n                break\n        if candidate is not None:\n            execution_engine.offer(member, candidate, True)\n            outcome = {'vitality': 5, 'cargo': -5}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'expand':\n        # Attempt to expand our territory. Choose any other member (as a proxy for a location/neighbor).\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                candidate = other\n                break\n        if candidate is not None:\n            execution_engine.expand(member, candidate)\n            outcome = {'vitality': 0, 'cargo': 10}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'bear':\n        # Reproduction action: only proceed if we have land.\n        # We assume that having a non-empty current_clear_list is a proxy for having land.\n        if len(member.current_clear_list) > 0:\n            # Find a partner with sufficient vitality (and not yourself) to bear offspring.\n            candidate = None\n            for other in execution_engine.current_members:\n                if other.id != member.id and other.vitality > 20:\n                    candidate = other\n                    break\n            if candidate is not None:\n                execution_engine.bear(member, candidate)\n                outcome = {'vitality': -5, 'cargo': -5}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n        else:\n            # If we don't have land, try offering land instead.\n            candidate = None\n            for other in execution_engine.current_members:\n                if other.id != member.id:\n                    candidate = other\n                    break\n            if candidate is not None:\n                execution_engine.offer_land(member, candidate, True)\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    # Compute a reward that emphasizes survival.\n    reward = outcome['vitality'] * 0.7 + outcome['cargo'] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Q-learning update:\n    new_state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n    max_future = max([member.learning['q_table'].get((new_state, a), 0) for a in actions])\n    learning_rate = 0.1\n    discount_factor = 0.9\n    member.learning['q_table'][(state, action)] += learning_rate * (\n        reward + discount_factor * max_future - member.learning['q_table'][(state, action)]\n    )\n\n    # Update learning logs and survival metrics.\n    member.learning['strategy_log'].append((action, outcome, reward))\n    member.learning['state_history'].append(state)\n    member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n    member.learning['survival_metrics']['success_rates'][action].append(1 if reward > 0 else 0)",
        "features_at_generation": [
          {
            "self_productivity": 9.851974486110818,
            "self_vitality": 73.85382847590404,
            "self_cargo": 43.51277128714192,
            "self_age": 90,
            "self_neighbor": 3,
            "member_id": 0
          },
          {
            "self_productivity": 8.077967059825381,
            "self_vitality": 33.02781433323713,
            "self_cargo": 47.25322455383092,
            "self_age": 81,
            "self_neighbor": 3,
            "member_id": 1
          },
          {
            "self_productivity": 7.968737200820967,
            "self_vitality": 42.15604281108604,
            "self_cargo": 40.25508270288947,
            "self_age": 29,
            "self_neighbor": 3,
            "member_id": 2
          },
          {
            "self_productivity": 9.205662089622338,
            "self_vitality": 18.47747982181767,
            "self_cargo": 46.68166847941894,
            "self_age": 24,
            "self_neighbor": 3,
            "member_id": 3
          },
          {
            "self_productivity": 7.799514361095985,
            "self_vitality": 78.50637096028503,
            "self_cargo": 42.476807026876735,
            "self_age": 92,
            "self_neighbor": 0,
            "member_id": 4
          }
        ],
        "relationships_at_generation": [
          "member_0 was attacked by member_1 (value=-15.67)",
          "member_0 was attacked by member_2 (value=38.40)",
          "member_0 was attacked by member_3 (value=29.62)",
          "member_0 was attacked by member_4 (value=50.87)",
          "member_1 was attacked by member_0 (value=34.45)",
          "member_1 was attacked by member_2 (value=-37.18)",
          "member_1 was attacked by member_3 (value=-44.22)",
          "member_1 was attacked by member_4 (value=23.17)",
          "member_2 was attacked by member_0 (value=85.84)",
          "member_2 was attacked by member_1 (value=28.99)",
          "member_2 was attacked by member_3 (value=49.52)",
          "member_2 was attacked by member_4 (value=18.07)",
          "member_3 was attacked by member_0 (value=44.24)",
          "member_3 was attacked by member_1 (value=65.23)",
          "member_3 was attacked by member_2 (value=-38.21)",
          "member_3 was attacked by member_4 (value=9.15)",
          "member_4 was attacked by member_0 (value=12.29)",
          "member_4 was attacked by member_1 (value=-45.17)",
          "member_4 was attacked by member_2 (value=-8.52)",
          "member_4 was attacked by member_3 (value=-26.54)",
          "member_0 gave a benefit to member_1 (value=62.61)",
          "member_0 gave a benefit to member_2 (value=56.24)",
          "member_0 gave a benefit to member_3 (value=-23.41)",
          "member_0 gave a benefit to member_4 (value=-41.80)",
          "member_1 gave a benefit to member_0 (value=74.37)",
          "member_1 gave a benefit to member_2 (value=-31.93)",
          "member_1 gave a benefit to member_3 (value=-0.25)",
          "member_1 gave a benefit to member_4 (value=16.10)",
          "member_2 gave a benefit to member_0 (value=-48.52)",
          "member_2 gave a benefit to member_1 (value=39.57)",
          "member_2 gave a benefit to member_3 (value=26.33)",
          "member_2 gave a benefit to member_4 (value=95.19)",
          "member_3 gave a benefit to member_0 (value=96.55)",
          "member_3 gave a benefit to member_1 (value=14.38)",
          "member_3 gave a benefit to member_2 (value=-2.61)",
          "member_3 gave a benefit to member_4 (value=89.48)",
          "member_4 gave a benefit to member_0 (value=50.22)",
          "member_4 gave a benefit to member_1 (value=41.71)",
          "member_4 gave a benefit to member_2 (value=-0.75)",
          "member_4 gave a benefit to member_3 (value=67.34)",
          "member_0 gave land to member_1 (value=2.89)",
          "member_0 gave land to member_2 (value=-2.30)",
          "member_0 gave land to member_3 (value=-1.82)",
          "member_0 gave land to member_4 (value=-2.79)",
          "member_1 gave land to member_0 (value=-0.60)",
          "member_1 gave land to member_2 (value=-0.64)",
          "member_1 gave land to member_3 (value=-1.34)",
          "member_1 gave land to member_4 (value=-2.00)",
          "member_2 gave land to member_0 (value=1.23)",
          "member_2 gave land to member_1 (value=-0.39)",
          "member_2 gave land to member_3 (value=2.10)",
          "member_2 gave land to member_4 (value=2.63)",
          "member_3 gave land to member_0 (value=-0.20)",
          "member_3 gave land to member_1 (value=1.36)",
          "member_3 gave land to member_2 (value=2.23)",
          "member_3 gave land to member_4 (value=1.30)",
          "member_4 gave land to member_0 (value=0.18)",
          "member_4 gave land to member_1 (value=-1.38)",
          "member_4 gave land to member_2 (value=-2.79)",
          "member_4 gave land to member_3 (value=0.64)"
        ]
      },
      "1": {
        "code": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Ensure member index exists\n    if member_id >= len(execution_engine.current_members):\n        return\n\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\n            \"q_table\": defaultdict(float),\n            \"strategy_log\": [],\n            \"state_history\": [],\n            \"survival_metrics\": {\n                \"vitality_changes\": [],\n                \"resource_efficiency\": [],\n                \"threat_responses\": [],\n                \"success_rates\": defaultdict(list),\n            },\n        }\n\n    # Emergency resource management: if cargo is less than half of vitality,\n    # try to attack a member with more cargo.\n    if member.cargo < member.vitality * 0.5:\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                # Record a negative outcome for vitality loss and slight cargo gain\n                # then break to avoid multiple attacks in one round.\n                break\n\n    # Define current state: use vitality, cargo, and the number of cleared land adjacent areas\n    state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n\n    # Define action space.\n    # Available actions: offer resources, expand territory, attack another member.\n    # Add bear (reproduction) if the member appears to have land (non-empty clear list).\n    actions = [\"offer\", \"expand\", \"attack\"]\n    if len(member.current_clear_list) > 0:\n        actions.append(\"bear\")\n\n    # \u03b5-greedy selection: with probability epsilon choose a random action; else choose best Q-value action.\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        chosen_action = np.random.choice(actions)\n    else:\n        best_val = None\n        chosen_action = None\n        for action in actions:\n            q_val = member.learning[\"q_table\"].get((state, action), 0)\n            if best_val is None or q_val > best_val:\n                best_val = q_val\n                chosen_action = action\n\n    # Initialize outcome dictionary to track simulated changes\n    outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Execute chosen action\n    if chosen_action == \"attack\":\n        # Find a valid target: one that exists and is not self.\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                target = other\n                break\n        if target is not None:\n            execution_engine.attack(member, target)\n            outcome = {\"vitality\": -5, \"cargo\": 5}\n    elif chosen_action == \"offer\":\n        # Identify a target for cooperation, choosing the first valid neighbor.\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.offer(member, target, True)\n            outcome = {\"vitality\": 2, \"cargo\": -2}\n    elif chosen_action == \"expand\":\n        # Expand territory by interacting with another member.\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.expand(member, target)\n            outcome = {\"vitality\": 0, \"cargo\": 3}\n    elif chosen_action == \"bear\":\n        # Verify that member has land (using current_clear_list as a proxy for available land)\n        if len(member.current_clear_list) > 0:\n            # Choose a partner: a member with land (non-empty clear list) and is not self.\n            possible_partners = [\n                other\n                for other in execution_engine.current_members\n                if other.id != member.id and len(other.current_clear_list) > 0\n            ]\n            if possible_partners:\n                partner = np.random.choice(possible_partners)\n                execution_engine.bear(member, partner)\n                outcome = {\"vitality\": 3, \"cargo\": 0}\n\n    # Q-learning update\n    new_state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n    max_future = max([member.learning[\"q_table\"].get((new_state, a), 0) for a in actions])\n    learning_rate = 0.1\n    discount_factor = 0.9\n    old_value = member.learning[\"q_table\"].get((state, chosen_action), 0)\n    reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n    member.learning[\"q_table\"][(state, chosen_action)] = old_value + learning_rate * (\n        reward + discount_factor * max_future - old_value\n    )\n\n    # Update learning logs and survival metrics\n    member.learning[\"strategy_log\"].append((chosen_action, outcome, reward))\n    member.learning[\"state_history\"].append(state)\n    member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n    member.learning[\"survival_metrics\"][\"success_rates\"][chosen_action].append(1 if reward > 0 else 0)",
        "features_at_generation": [
          {
            "self_productivity": 9.851974486110818,
            "self_vitality": 73.85382847590404,
            "self_cargo": 43.51277128714192,
            "self_age": 90,
            "self_neighbor": 3,
            "member_id": 0
          },
          {
            "self_productivity": 8.077967059825381,
            "self_vitality": 33.02781433323713,
            "self_cargo": 47.25322455383092,
            "self_age": 81,
            "self_neighbor": 3,
            "member_id": 1
          },
          {
            "self_productivity": 7.968737200820967,
            "self_vitality": 42.15604281108604,
            "self_cargo": 40.25508270288947,
            "self_age": 29,
            "self_neighbor": 3,
            "member_id": 2
          },
          {
            "self_productivity": 9.205662089622338,
            "self_vitality": 18.47747982181767,
            "self_cargo": 46.68166847941894,
            "self_age": 24,
            "self_neighbor": 3,
            "member_id": 3
          },
          {
            "self_productivity": 7.799514361095985,
            "self_vitality": 78.50637096028503,
            "self_cargo": 42.476807026876735,
            "self_age": 92,
            "self_neighbor": 0,
            "member_id": 4
          }
        ],
        "relationships_at_generation": [
          "member_0 was attacked by member_1 (value=-15.67)",
          "member_0 was attacked by member_2 (value=38.40)",
          "member_0 was attacked by member_3 (value=29.62)",
          "member_0 was attacked by member_4 (value=50.87)",
          "member_1 was attacked by member_0 (value=34.45)",
          "member_1 was attacked by member_2 (value=-37.18)",
          "member_1 was attacked by member_3 (value=-44.22)",
          "member_1 was attacked by member_4 (value=23.17)",
          "member_2 was attacked by member_0 (value=85.84)",
          "member_2 was attacked by member_1 (value=28.99)",
          "member_2 was attacked by member_3 (value=49.52)",
          "member_2 was attacked by member_4 (value=18.07)",
          "member_3 was attacked by member_0 (value=44.24)",
          "member_3 was attacked by member_1 (value=65.23)",
          "member_3 was attacked by member_2 (value=-38.21)",
          "member_3 was attacked by member_4 (value=9.15)",
          "member_4 was attacked by member_0 (value=12.29)",
          "member_4 was attacked by member_1 (value=-45.17)",
          "member_4 was attacked by member_2 (value=-8.52)",
          "member_4 was attacked by member_3 (value=-26.54)",
          "member_0 gave a benefit to member_1 (value=62.61)",
          "member_0 gave a benefit to member_2 (value=56.24)",
          "member_0 gave a benefit to member_3 (value=-23.41)",
          "member_0 gave a benefit to member_4 (value=-41.80)",
          "member_1 gave a benefit to member_0 (value=74.37)",
          "member_1 gave a benefit to member_2 (value=-31.93)",
          "member_1 gave a benefit to member_3 (value=-0.25)",
          "member_1 gave a benefit to member_4 (value=16.10)",
          "member_2 gave a benefit to member_0 (value=-48.52)",
          "member_2 gave a benefit to member_1 (value=39.57)",
          "member_2 gave a benefit to member_3 (value=26.33)",
          "member_2 gave a benefit to member_4 (value=95.19)",
          "member_3 gave a benefit to member_0 (value=96.55)",
          "member_3 gave a benefit to member_1 (value=14.38)",
          "member_3 gave a benefit to member_2 (value=-2.61)",
          "member_3 gave a benefit to member_4 (value=89.48)",
          "member_4 gave a benefit to member_0 (value=50.22)",
          "member_4 gave a benefit to member_1 (value=41.71)",
          "member_4 gave a benefit to member_2 (value=-0.75)",
          "member_4 gave a benefit to member_3 (value=67.34)",
          "member_0 gave land to member_1 (value=2.89)",
          "member_0 gave land to member_2 (value=-2.30)",
          "member_0 gave land to member_3 (value=-1.82)",
          "member_0 gave land to member_4 (value=-2.79)",
          "member_1 gave land to member_0 (value=-0.60)",
          "member_1 gave land to member_2 (value=-0.64)",
          "member_1 gave land to member_3 (value=-1.34)",
          "member_1 gave land to member_4 (value=-2.00)",
          "member_2 gave land to member_0 (value=1.23)",
          "member_2 gave land to member_1 (value=-0.39)",
          "member_2 gave land to member_3 (value=2.10)",
          "member_2 gave land to member_4 (value=2.63)",
          "member_3 gave land to member_0 (value=-0.20)",
          "member_3 gave land to member_1 (value=1.36)",
          "member_3 gave land to member_2 (value=2.23)",
          "member_3 gave land to member_4 (value=1.30)",
          "member_4 gave land to member_0 (value=0.18)",
          "member_4 gave land to member_1 (value=-1.38)",
          "member_4 gave land to member_2 (value=-2.79)",
          "member_4 gave land to member_3 (value=0.64)"
        ]
      },
      "2": {
        "code": "def agent_action(execution_engine, member_id):\n    # Ensure the member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning/log structure if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\"log\": []}\n\n    # Emergency resource management: if cargo is less than half of vitality,\n    # look for a target (with more cargo than self) and attack them.\n    if member.cargo < 0.5 * member.vitality:\n        target = None\n        max_cargo = -1\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                if other.cargo > max_cargo:\n                    max_cargo = other.cargo\n                    target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            member.learning[\"log\"].append(\"Attacked member_%d for resource emergency\" % target.id)\n            return\n\n    # Build alliances and cooperative networks:\n    # Offer resources to members with lower vitality to forge alliances.\n    for other in execution_engine.current_members:\n        if other.id != member.id and other.vitality < member.vitality:\n            execution_engine.offer(member, other, True)\n            member.learning[\"log\"].append(\"Offered resources to member_%d for alliance\" % other.id)\n\n    # Expand territory: choose a neighbor by index (member.id + 1) if that index exists.\n    neighbor_index = member.id + 1\n    if neighbor_index < len(execution_engine.current_members):\n        target = execution_engine.current_members[neighbor_index]\n        if target.id != member.id:\n            execution_engine.expand(member, target)\n            member.learning[\"log\"].append(\"Expanded territory near member_%d\" % target.id)\n\n    # Repopulation / social growth: if member appears to have land, then attempt to bear offspring.\n    # We check for existence of an attribute \"land_num\" and that it is > 0.\n    if hasattr(member, \"land_num\") and member.land_num > 0:\n        # Find a partner with land and not self.\n        for partner in execution_engine.current_members:\n            if partner.id != member.id and hasattr(partner, \"land_num\") and partner.land_num > 0:\n                execution_engine.bear(member, partner)\n                member.learning[\"log\"].append(\"Bore offspring with member_%d\" % partner.id)\n                break\n\n    # Communication: Propose a communal basic income system to all other members\n    for other in execution_engine.current_members:\n        if other.id != member.id:\n            execution_engine.send_message(member.id, other.id,\n                \"Let's establish a basic income system to support resource sharing and mutual benefit.\")\n    member.learning[\"log\"].append(\"Sent basic income proposals to all other members\")",
        "features_at_generation": [
          {
            "self_productivity": 9.851974486110818,
            "self_vitality": 73.85382847590404,
            "self_cargo": 43.51277128714192,
            "self_age": 90,
            "self_neighbor": 3,
            "member_id": 0
          },
          {
            "self_productivity": 8.077967059825381,
            "self_vitality": 33.02781433323713,
            "self_cargo": 47.25322455383092,
            "self_age": 81,
            "self_neighbor": 3,
            "member_id": 1
          },
          {
            "self_productivity": 7.968737200820967,
            "self_vitality": 42.15604281108604,
            "self_cargo": 40.25508270288947,
            "self_age": 29,
            "self_neighbor": 3,
            "member_id": 2
          },
          {
            "self_productivity": 9.205662089622338,
            "self_vitality": 18.47747982181767,
            "self_cargo": 46.68166847941894,
            "self_age": 24,
            "self_neighbor": 3,
            "member_id": 3
          },
          {
            "self_productivity": 7.799514361095985,
            "self_vitality": 78.50637096028503,
            "self_cargo": 42.476807026876735,
            "self_age": 92,
            "self_neighbor": 0,
            "member_id": 4
          }
        ],
        "relationships_at_generation": [
          "member_0 was attacked by member_1 (value=-15.67)",
          "member_0 was attacked by member_2 (value=38.40)",
          "member_0 was attacked by member_3 (value=29.62)",
          "member_0 was attacked by member_4 (value=50.87)",
          "member_1 was attacked by member_0 (value=34.45)",
          "member_1 was attacked by member_2 (value=-37.18)",
          "member_1 was attacked by member_3 (value=-44.22)",
          "member_1 was attacked by member_4 (value=23.17)",
          "member_2 was attacked by member_0 (value=85.84)",
          "member_2 was attacked by member_1 (value=28.99)",
          "member_2 was attacked by member_3 (value=49.52)",
          "member_2 was attacked by member_4 (value=18.07)",
          "member_3 was attacked by member_0 (value=44.24)",
          "member_3 was attacked by member_1 (value=65.23)",
          "member_3 was attacked by member_2 (value=-38.21)",
          "member_3 was attacked by member_4 (value=9.15)",
          "member_4 was attacked by member_0 (value=12.29)",
          "member_4 was attacked by member_1 (value=-45.17)",
          "member_4 was attacked by member_2 (value=-8.52)",
          "member_4 was attacked by member_3 (value=-26.54)",
          "member_0 gave a benefit to member_1 (value=62.61)",
          "member_0 gave a benefit to member_2 (value=56.24)",
          "member_0 gave a benefit to member_3 (value=-23.41)",
          "member_0 gave a benefit to member_4 (value=-41.80)",
          "member_1 gave a benefit to member_0 (value=74.37)",
          "member_1 gave a benefit to member_2 (value=-31.93)",
          "member_1 gave a benefit to member_3 (value=-0.25)",
          "member_1 gave a benefit to member_4 (value=16.10)",
          "member_2 gave a benefit to member_0 (value=-48.52)",
          "member_2 gave a benefit to member_1 (value=39.57)",
          "member_2 gave a benefit to member_3 (value=26.33)",
          "member_2 gave a benefit to member_4 (value=95.19)",
          "member_3 gave a benefit to member_0 (value=96.55)",
          "member_3 gave a benefit to member_1 (value=14.38)",
          "member_3 gave a benefit to member_2 (value=-2.61)",
          "member_3 gave a benefit to member_4 (value=89.48)",
          "member_4 gave a benefit to member_0 (value=50.22)",
          "member_4 gave a benefit to member_1 (value=41.71)",
          "member_4 gave a benefit to member_2 (value=-0.75)",
          "member_4 gave a benefit to member_3 (value=67.34)",
          "member_0 gave land to member_1 (value=2.89)",
          "member_0 gave land to member_2 (value=-2.30)",
          "member_0 gave land to member_3 (value=-1.82)",
          "member_0 gave land to member_4 (value=-2.79)",
          "member_1 gave land to member_0 (value=-0.60)",
          "member_1 gave land to member_2 (value=-0.64)",
          "member_1 gave land to member_3 (value=-1.34)",
          "member_1 gave land to member_4 (value=-2.00)",
          "member_2 gave land to member_0 (value=1.23)",
          "member_2 gave land to member_1 (value=-0.39)",
          "member_2 gave land to member_3 (value=2.10)",
          "member_2 gave land to member_4 (value=2.63)",
          "member_3 gave land to member_0 (value=-0.20)",
          "member_3 gave land to member_1 (value=1.36)",
          "member_3 gave land to member_2 (value=2.23)",
          "member_3 gave land to member_4 (value=1.30)",
          "member_4 gave land to member_0 (value=0.18)",
          "member_4 gave land to member_1 (value=-1.38)",
          "member_4 gave land to member_2 (value=-2.79)",
          "member_4 gave land to member_3 (value=0.64)"
        ]
      },
      "3": {
        "code": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Ensure member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structure if not present\n    if not hasattr(member, 'learning'):\n        member.learning = {\n            'q_table': defaultdict(float),  # Q-values for state-action pairs\n            'strategy_log': [],            # Log of (action, outcome, reward)\n            'state_history': [],           # History of states\n            'survival_metrics': {\n                'vitality_changes': [],\n                'resource_efficiency': [],\n                'threat_responses': [],\n                'success_rates': defaultdict(list)\n            }\n        }\n\n    # Emergency resource management: if cargo is less than half of vitality,\n    # attempt to attack a member with higher cargo (making sure not to target self)\n    if member.cargo < member.vitality * 0.5:\n        for other in execution_engine.current_members:\n            # Ensure we do not target ourselves\n            if other.id == member.id:\n                continue\n            # Attack a target with more cargo than you\n            if other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                # Simulated outcome for logging purposes\n                outcome = {\"vitality\": -5, \"cargo\": 5}\n                state = (int(member.vitality)//10, int(member.cargo)//10, len(member.current_clear_list))\n                member.learning['strategy_log'].append((\"attack\", outcome, None))\n                member.learning['state_history'].append(state)\n                return\n\n    # Define the set of potential actions\n    actions = ['offer', 'expand', 'bear', 'message']\n    # Remove 'bear' action if member does not have any land (verify with attribute 'owned_land')\n    if not (hasattr(member, 'owned_land') and member.owned_land):\n        if 'bear' in actions:\n            actions.remove('bear')\n\n    # Determine current state metrics (using integer division for discretization)\n    state = (int(member.vitality)//10, int(member.cargo)//10, len(member.current_clear_list))\n    epsilon = 0.1  # \u03b5 for \u03b5-greedy strategy\n\n    # Choose action using \u03b5-greedy policy\n    if np.random.rand() < epsilon:\n        action = np.random.choice(actions)\n    else:\n        best_action = None\n        best_val = -1e9\n        for a in actions:\n            val = member.learning['q_table'][(state, a)]\n            if val > best_val:\n                best_val = val\n                best_action = a\n        action = best_action if best_action is not None else np.random.choice(actions)\n\n    # Execute the chosen action and record a simulated outcome.\n    outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    if action == 'offer':\n        # Find a candidate member (not self) and offer resources.\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.offer(member, other, True)\n            outcome = {\"vitality\": 2, \"cargo\": -3}  # example outcome effect\n            break\n\n    elif action == 'expand':\n        # Expand territory by targeting a neighbor from the clear list if possible.\n        if member.current_clear_list:\n            target_index = member.current_clear_list[0]\n            if target_index < len(execution_engine.current_members):\n                target = execution_engine.current_members[target_index]\n                execution_engine.expand(member, target)\n                outcome = {\"vitality\": 1, \"cargo\": -1}\n\n    elif action == 'bear':\n        # Bear offspring with the first valid partner (not yourself)\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.bear(member, other)\n            outcome = {\"vitality\": -2, \"cargo\": 0}\n            break\n\n    elif action == 'message':\n        # Send a coalition-building message to all other members\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.send_message(member.id, other.id,\n                \"Let's build a coalition for resource sharing and collective decision making.\")\n        outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Calculate reward with a focus on survival:\n    reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Determine the new state after action (for learning update)\n    new_state = (int(member.vitality)//10, int(member.cargo)//10, len(member.current_clear_list))\n    # Evaluate maximum Q value for next state over all possible actions\n    max_future = max([member.learning['q_table'][(new_state, a)] for a in actions] or [0])\n    # Q-learning update\n    member.learning['q_table'][(state, action)] += 0.1 * (\n        reward + 0.9 * max_future - member.learning['q_table'][(state, action)]\n    )\n\n    # Log the outcome and survival metrics\n    member.learning['strategy_log'].append((action, outcome, reward))\n    member.learning['state_history'].append(new_state)\n    member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n    member.learning['survival_metrics']['success_rates'][action].append(1 if reward > 0 else 0)",
        "features_at_generation": [
          {
            "self_productivity": 9.851974486110818,
            "self_vitality": 73.85382847590404,
            "self_cargo": 43.51277128714192,
            "self_age": 90,
            "self_neighbor": 3,
            "member_id": 0
          },
          {
            "self_productivity": 8.077967059825381,
            "self_vitality": 33.02781433323713,
            "self_cargo": 47.25322455383092,
            "self_age": 81,
            "self_neighbor": 3,
            "member_id": 1
          },
          {
            "self_productivity": 7.968737200820967,
            "self_vitality": 42.15604281108604,
            "self_cargo": 40.25508270288947,
            "self_age": 29,
            "self_neighbor": 3,
            "member_id": 2
          },
          {
            "self_productivity": 9.205662089622338,
            "self_vitality": 18.47747982181767,
            "self_cargo": 46.68166847941894,
            "self_age": 24,
            "self_neighbor": 3,
            "member_id": 3
          },
          {
            "self_productivity": 7.799514361095985,
            "self_vitality": 78.50637096028503,
            "self_cargo": 42.476807026876735,
            "self_age": 92,
            "self_neighbor": 0,
            "member_id": 4
          }
        ],
        "relationships_at_generation": [
          "member_0 was attacked by member_1 (value=-15.67)",
          "member_0 was attacked by member_2 (value=38.40)",
          "member_0 was attacked by member_3 (value=29.62)",
          "member_0 was attacked by member_4 (value=50.87)",
          "member_1 was attacked by member_0 (value=34.45)",
          "member_1 was attacked by member_2 (value=-37.18)",
          "member_1 was attacked by member_3 (value=-44.22)",
          "member_1 was attacked by member_4 (value=23.17)",
          "member_2 was attacked by member_0 (value=85.84)",
          "member_2 was attacked by member_1 (value=28.99)",
          "member_2 was attacked by member_3 (value=49.52)",
          "member_2 was attacked by member_4 (value=18.07)",
          "member_3 was attacked by member_0 (value=44.24)",
          "member_3 was attacked by member_1 (value=65.23)",
          "member_3 was attacked by member_2 (value=-38.21)",
          "member_3 was attacked by member_4 (value=9.15)",
          "member_4 was attacked by member_0 (value=12.29)",
          "member_4 was attacked by member_1 (value=-45.17)",
          "member_4 was attacked by member_2 (value=-8.52)",
          "member_4 was attacked by member_3 (value=-26.54)",
          "member_0 gave a benefit to member_1 (value=62.61)",
          "member_0 gave a benefit to member_2 (value=56.24)",
          "member_0 gave a benefit to member_3 (value=-23.41)",
          "member_0 gave a benefit to member_4 (value=-41.80)",
          "member_1 gave a benefit to member_0 (value=74.37)",
          "member_1 gave a benefit to member_2 (value=-31.93)",
          "member_1 gave a benefit to member_3 (value=-0.25)",
          "member_1 gave a benefit to member_4 (value=16.10)",
          "member_2 gave a benefit to member_0 (value=-48.52)",
          "member_2 gave a benefit to member_1 (value=39.57)",
          "member_2 gave a benefit to member_3 (value=26.33)",
          "member_2 gave a benefit to member_4 (value=95.19)",
          "member_3 gave a benefit to member_0 (value=96.55)",
          "member_3 gave a benefit to member_1 (value=14.38)",
          "member_3 gave a benefit to member_2 (value=-2.61)",
          "member_3 gave a benefit to member_4 (value=89.48)",
          "member_4 gave a benefit to member_0 (value=50.22)",
          "member_4 gave a benefit to member_1 (value=41.71)",
          "member_4 gave a benefit to member_2 (value=-0.75)",
          "member_4 gave a benefit to member_3 (value=67.34)",
          "member_0 gave land to member_1 (value=2.89)",
          "member_0 gave land to member_2 (value=-2.30)",
          "member_0 gave land to member_3 (value=-1.82)",
          "member_0 gave land to member_4 (value=-2.79)",
          "member_1 gave land to member_0 (value=-0.60)",
          "member_1 gave land to member_2 (value=-0.64)",
          "member_1 gave land to member_3 (value=-1.34)",
          "member_1 gave land to member_4 (value=-2.00)",
          "member_2 gave land to member_0 (value=1.23)",
          "member_2 gave land to member_1 (value=-0.39)",
          "member_2 gave land to member_3 (value=2.10)",
          "member_2 gave land to member_4 (value=2.63)",
          "member_3 gave land to member_0 (value=-0.20)",
          "member_3 gave land to member_1 (value=1.36)",
          "member_3 gave land to member_2 (value=2.23)",
          "member_3 gave land to member_4 (value=1.30)",
          "member_4 gave land to member_0 (value=0.18)",
          "member_4 gave land to member_1 (value=-1.38)",
          "member_4 gave land to member_2 (value=-2.79)",
          "member_4 gave land to member_3 (value=0.64)"
        ]
      },
      "4": {
        "code": "import numpy as np\nfrom collections import defaultdict\n\ndef agent_action(execution_engine, member_id):\n    # Get our member; ensure member_id index is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already present\n    if not hasattr(member, 'learning'):\n        member.learning = {\n            'q_table': defaultdict(float),  # Q-value table: key = (state, action)\n            'strategy_log': [],             # Log of (action, outcome, reward)\n            'state_history': [],            # History of states\n            'survival_metrics': {\n                'vitality_changes': [],\n                'resource_efficiency': [],\n                'threat_responses': [],\n                'success_rates': defaultdict(list)\n            }\n        }\n\n    # Emergency resource management: if cargo is low compared to vitality, try to attack a richer member.\n    if member.cargo < member.vitality * 0.5:\n        for other in execution_engine.current_members:\n            # Ensure valid member and do not target self.\n            if other.id != member.id and other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                # Only attack one opponent for emergency resource management.\n                break\n\n    # Adaptive territory defense: if member has land and is vulnerable, discard some extra land.\n    if hasattr(member, \"land_num\") and member.land_num > 2 and member.vitality < 50:\n        if hasattr(member, \"owned_land\"):\n            for loc in member.owned_land[1:]:\n                # Assuming _discard_land is a valid method.\n                execution_engine._discard_land(member, loc)\n\n    # Define current state as a tuple based on vitality, cargo, and cleared neighbor list.\n    state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n\n    # Define possible actions (social strategies) we can perform.\n    actions = ['attack', 'offer', 'expand', 'bear']\n\n    # Epsilon-greedy strategy: with probability 0.1 choose a random action.\n    if np.random.rand() < 0.1:\n        action = np.random.choice(actions)\n    else:\n        # Choose action with highest Q-value, defaulting to 0 if not present.\n        action = max(actions, key=lambda a: member.learning['q_table'][(state, a)])\n\n    # Define helper function to execute the chosen action and simulate outcome.\n    def execute_action(chosen_action):\n        outcome = {'vitality': 0, 'cargo': 0}\n        # Attack: find a target with higher cargo.\n        if chosen_action == 'attack':\n            target = None\n            for candidate in execution_engine.current_members:\n                # Ensure valid index and do not target self.\n                if candidate.id != member.id and candidate.cargo > member.cargo:\n                    target = candidate\n                    break\n            if target:\n                execution_engine.attack(member, target)\n                # Outcome simulation: slight vitality cost, cargo gain.\n                outcome = {'vitality': -5, 'cargo': 10}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n\n        # Offer: offer resources to a member who might benefit from help.\n        elif chosen_action == 'offer':\n            target = None\n            for candidate in execution_engine.current_members:\n                if candidate.id != member.id and candidate.cargo < member.cargo:\n                    target = candidate\n                    break\n            if target:\n                execution_engine.offer(member, target, True)\n                # Outcome simulation: small resource cost.\n                outcome = {'vitality': 0, 'cargo': -3}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n\n        # Expand: attempt to expand territory by interacting with another member.\n        elif chosen_action == 'expand':\n            target = None\n            for candidate in execution_engine.current_members:\n                if candidate.id != member.id:\n                    target = candidate\n                    break\n            if target:\n                execution_engine.expand(member, target)\n                # Outcome simulation: slight cost in vitality and a gain in cargo.\n                outcome = {'vitality': -2, 'cargo': 5}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n\n        # Bear: produce offspring, but only if member has land.\n        elif chosen_action == 'bear':\n            if hasattr(member, \"land_num\") and member.land_num > 0:\n                partner = None\n                for candidate in execution_engine.current_members:\n                    # Ensure not targeting self and partner also has some land.\n                    if candidate.id != member.id and hasattr(candidate, \"land_num\") and candidate.land_num > 0:\n                        partner = candidate\n                        break\n                if partner:\n                    execution_engine.bear(member, partner)\n                    # Outcome simulation: cost in vitality.\n                    outcome = {'vitality': -3, 'cargo': 0}\n                else:\n                    outcome = {'vitality': 0, 'cargo': 0}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n        return outcome\n\n    # Execute the chosen action and get the outcome.\n    outcome = execute_action(action)\n\n    # Compute a reward that emphasizes survival (vitality) and resource (cargo) management.\n    reward = outcome['vitality'] * 0.7 + outcome['cargo'] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Update Q-learning: observe new state after action.\n    new_state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n    max_future = max(member.learning['q_table'][(new_state, a)] for a in actions)\n    learning_rate = 0.1\n    discount_factor = 0.9\n    member.learning['q_table'][(state, action)] += learning_rate * (\n        reward + discount_factor * max_future - member.learning['q_table'][(state, action)]\n    )\n\n    # Update learning logs\n    member.learning['strategy_log'].append((action, outcome, reward))\n    member.learning['state_history'].append(state)\n    member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n    member.learning['survival_metrics']['success_rates'][action].append(1 if reward > 0 else 0)",
        "features_at_generation": [
          {
            "self_productivity": 9.851974486110818,
            "self_vitality": 73.85382847590404,
            "self_cargo": 43.51277128714192,
            "self_age": 90,
            "self_neighbor": 3,
            "member_id": 0
          },
          {
            "self_productivity": 8.077967059825381,
            "self_vitality": 33.02781433323713,
            "self_cargo": 47.25322455383092,
            "self_age": 81,
            "self_neighbor": 3,
            "member_id": 1
          },
          {
            "self_productivity": 7.968737200820967,
            "self_vitality": 42.15604281108604,
            "self_cargo": 40.25508270288947,
            "self_age": 29,
            "self_neighbor": 3,
            "member_id": 2
          },
          {
            "self_productivity": 9.205662089622338,
            "self_vitality": 18.47747982181767,
            "self_cargo": 46.68166847941894,
            "self_age": 24,
            "self_neighbor": 3,
            "member_id": 3
          },
          {
            "self_productivity": 7.799514361095985,
            "self_vitality": 78.50637096028503,
            "self_cargo": 42.476807026876735,
            "self_age": 92,
            "self_neighbor": 0,
            "member_id": 4
          }
        ],
        "relationships_at_generation": [
          "member_0 was attacked by member_1 (value=-15.67)",
          "member_0 was attacked by member_2 (value=38.40)",
          "member_0 was attacked by member_3 (value=29.62)",
          "member_0 was attacked by member_4 (value=50.87)",
          "member_1 was attacked by member_0 (value=34.45)",
          "member_1 was attacked by member_2 (value=-37.18)",
          "member_1 was attacked by member_3 (value=-44.22)",
          "member_1 was attacked by member_4 (value=23.17)",
          "member_2 was attacked by member_0 (value=85.84)",
          "member_2 was attacked by member_1 (value=28.99)",
          "member_2 was attacked by member_3 (value=49.52)",
          "member_2 was attacked by member_4 (value=18.07)",
          "member_3 was attacked by member_0 (value=44.24)",
          "member_3 was attacked by member_1 (value=65.23)",
          "member_3 was attacked by member_2 (value=-38.21)",
          "member_3 was attacked by member_4 (value=9.15)",
          "member_4 was attacked by member_0 (value=12.29)",
          "member_4 was attacked by member_1 (value=-45.17)",
          "member_4 was attacked by member_2 (value=-8.52)",
          "member_4 was attacked by member_3 (value=-26.54)",
          "member_0 gave a benefit to member_1 (value=62.61)",
          "member_0 gave a benefit to member_2 (value=56.24)",
          "member_0 gave a benefit to member_3 (value=-23.41)",
          "member_0 gave a benefit to member_4 (value=-41.80)",
          "member_1 gave a benefit to member_0 (value=74.37)",
          "member_1 gave a benefit to member_2 (value=-31.93)",
          "member_1 gave a benefit to member_3 (value=-0.25)",
          "member_1 gave a benefit to member_4 (value=16.10)",
          "member_2 gave a benefit to member_0 (value=-48.52)",
          "member_2 gave a benefit to member_1 (value=39.57)",
          "member_2 gave a benefit to member_3 (value=26.33)",
          "member_2 gave a benefit to member_4 (value=95.19)",
          "member_3 gave a benefit to member_0 (value=96.55)",
          "member_3 gave a benefit to member_1 (value=14.38)",
          "member_3 gave a benefit to member_2 (value=-2.61)",
          "member_3 gave a benefit to member_4 (value=89.48)",
          "member_4 gave a benefit to member_0 (value=50.22)",
          "member_4 gave a benefit to member_1 (value=41.71)",
          "member_4 gave a benefit to member_2 (value=-0.75)",
          "member_4 gave a benefit to member_3 (value=67.34)",
          "member_0 gave land to member_1 (value=2.89)",
          "member_0 gave land to member_2 (value=-2.30)",
          "member_0 gave land to member_3 (value=-1.82)",
          "member_0 gave land to member_4 (value=-2.79)",
          "member_1 gave land to member_0 (value=-0.60)",
          "member_1 gave land to member_2 (value=-0.64)",
          "member_1 gave land to member_3 (value=-1.34)",
          "member_1 gave land to member_4 (value=-2.00)",
          "member_2 gave land to member_0 (value=1.23)",
          "member_2 gave land to member_1 (value=-0.39)",
          "member_2 gave land to member_3 (value=2.10)",
          "member_2 gave land to member_4 (value=2.63)",
          "member_3 gave land to member_0 (value=-0.20)",
          "member_3 gave land to member_1 (value=1.36)",
          "member_3 gave land to member_2 (value=2.23)",
          "member_3 gave land to member_4 (value=1.30)",
          "member_4 gave land to member_0 (value=0.18)",
          "member_4 gave land to member_1 (value=-1.38)",
          "member_4 gave land to member_2 (value=-2.79)",
          "member_4 gave land to member_3 (value=0.64)"
        ]
      }
    },
    "1": {
      "0": {
        "code": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Validate member_id index exists\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structure if absent\n    if not hasattr(member, 'learning'):\n        member.learning = {\n            'q_table': defaultdict(float),  # Maps (state, action) -> value\n            'strategy_log': [],            # List of tuples (action, outcome, reward)\n            'state_history': [],           # History of states\n            'survival_metrics': {\n                'vitality_changes': [],\n                'resource_efficiency': [],\n                'threat_responses': [],\n                'success_rates': defaultdict(list)\n            }\n        }\n\n    # Emergency resource management:\n    # If cargo is low relative to vitality, attempt to seize resources by attacking a member with high cargo.\n    if member.cargo < member.vitality * 0.5:\n        target = None\n        max_cargo = -float(\"inf\")\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > max_cargo:\n                max_cargo = other.cargo\n                target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            outcome = {'vitality': -10, 'cargo': member.cargo * 0.1}\n            state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n            member.learning['strategy_log'].append(('attack', outcome, None))\n            member.learning['state_history'].append(state)\n            member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n            member.learning['survival_metrics']['success_rates']['attack'].append(1 if outcome['vitality'] >= 0 else 0)\n            return\n\n    # Define available actions\n    actions = ['offer', 'expand', 'bear', 'communicate']\n    state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n\n    # Epsilon-greedy action selection (10% random)\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action = np.random.choice(actions)\n    else:\n        # Choose best action from q_table (defaulting to 0 if unseen)\n        action = max(actions, key=lambda a: member.learning['q_table'][(state, a)])\n\n    outcome = {'vitality': 0, 'cargo': 0}  # Default outcome\n\n    if action == 'offer':\n        # Propose a resource sharing offer to a member with lower cargo\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo:\n                candidate = other\n                break\n        if candidate is not None:\n            execution_engine.offer(member, candidate, True)\n            outcome = {'vitality': 3, 'cargo': -3}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'expand':\n        # Attempt to expand territory by interacting with a neighbor\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                candidate = other\n                break\n        if candidate is not None:\n            execution_engine.expand(member, candidate)\n            outcome = {'vitality': 0, 'cargo': 10}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'bear':\n        # Reproduction action: ensure member has land (non-empty current_clear_list)\n        if len(member.current_clear_list) > 0:\n            candidate = None\n            for other in execution_engine.current_members:\n                if other.id != member.id and other.vitality > 30:\n                    candidate = other\n                    break\n            if candidate is not None:\n                execution_engine.bear(member, candidate)\n                outcome = {'vitality': -5, 'cargo': -5}\n            else:\n                outcome = {'vitality': 0, 'cargo': 0}\n        else:\n            # If no land, offer land to a candidate as a proxy for territory sharing\n            candidate = None\n            for other in execution_engine.current_members:\n                if other.id != member.id:\n                    candidate = other\n                    break\n            if candidate is not None:\n                execution_engine.offer_land(member, candidate, True)\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    elif action == 'communicate':\n        # Use communication to form alliances: send a message to a neighbor with similar resources\n        candidate = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                # Selecting candidates with similar cargo (threshold arbitrarily chosen as 5 units)\n                if abs(other.cargo - member.cargo) < 5:\n                    candidate = other\n                    break\n        if candidate is not None:\n            execution_engine.send_message(member.id, candidate.id, \"Let's form an alliance for resource sharing.\")\n            outcome = {'vitality': 1, 'cargo': 0}\n        else:\n            outcome = {'vitality': 0, 'cargo': 0}\n\n    # Compute reward with a focus on survival: vitality and resource gains matter.\n    reward = outcome['vitality'] * 0.7 + outcome['cargo'] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Q-learning update: observe new state after action\n    new_state = (member.vitality // 10, member.cargo // 10, len(member.current_clear_list))\n    max_future = max([member.learning['q_table'][(new_state, a)] for a in actions])\n    learning_rate = 0.1\n    discount_factor = 0.9\n    member.learning['q_table'][(state, action)] += learning_rate * (reward + discount_factor * max_future - member.learning['q_table'][(state, action)])\n\n    # Update logs and survival metrics\n    member.learning['strategy_log'].append((action, outcome, reward))\n    member.learning['state_history'].append(state)\n    member.learning['survival_metrics']['vitality_changes'].append(outcome['vitality'])\n    member.learning['survival_metrics']['success_rates'][action].append(1 if reward > 0 else 0)",
        "features_at_generation": [
          {
            "self_productivity": 8.077967059825381,
            "self_vitality": 76.04422565143308,
            "self_cargo": 8.077967059825381,
            "self_age": 82,
            "self_neighbor": 2,
            "member_id": 1
          },
          {
            "self_productivity": 7.968737200820967,
            "self_vitality": 63.94123564803273,
            "self_cargo": 7.968737200820967,
            "self_age": 30,
            "self_neighbor": 2,
            "member_id": 2
          },
          {
            "self_productivity": 9.205662089622338,
            "self_vitality": 44.514396824537286,
            "self_cargo": 9.205662089622338,
            "self_age": 25,
            "self_neighbor": 2,
            "member_id": 3
          },
          {
            "self_productivity": 7.799514361095985,
            "self_vitality": 60.347944581208296,
            "self_cargo": 7.799514361095985,
            "self_age": 93,
            "self_neighbor": 0,
            "member_id": 4
          }
        ],
        "relationships_at_generation": [
          "member_1 was attacked by member_2 (value=-37.18)",
          "member_1 was attacked by member_3 (value=-44.22)",
          "member_1 was attacked by member_4 (value=23.17)",
          "member_2 was attacked by member_1 (value=28.99)",
          "member_2 was attacked by member_3 (value=49.52)",
          "member_2 was attacked by member_4 (value=18.07)",
          "member_3 was attacked by member_1 (value=65.23)",
          "member_3 was attacked by member_2 (value=-38.21)",
          "member_3 was attacked by member_4 (value=9.15)",
          "member_4 was attacked by member_1 (value=-45.17)",
          "member_4 was attacked by member_2 (value=-8.52)",
          "member_4 was attacked by member_3 (value=-26.54)",
          "member_1 gave a benefit to member_2 (value=-26.13)",
          "member_1 gave a benefit to member_3 (value=-0.25)",
          "member_1 gave a benefit to member_4 (value=16.10)",
          "member_2 gave a benefit to member_1 (value=39.57)",
          "member_2 gave a benefit to member_3 (value=26.33)",
          "member_2 gave a benefit to member_4 (value=95.19)",
          "member_3 gave a benefit to member_1 (value=14.38)",
          "member_3 gave a benefit to member_2 (value=6.34)",
          "member_3 gave a benefit to member_4 (value=89.48)",
          "member_4 gave a benefit to member_1 (value=41.71)",
          "member_4 gave a benefit to member_2 (value=-0.75)",
          "member_4 gave a benefit to member_3 (value=67.34)",
          "member_1 gave land to member_2 (value=-0.64)",
          "member_1 gave land to member_3 (value=-1.34)",
          "member_1 gave land to member_4 (value=-2.00)",
          "member_2 gave land to member_1 (value=-0.39)",
          "member_2 gave land to member_3 (value=2.10)",
          "member_2 gave land to member_4 (value=2.63)",
          "member_3 gave land to member_1 (value=1.36)",
          "member_3 gave land to member_2 (value=2.23)",
          "member_3 gave land to member_4 (value=1.30)",
          "member_4 gave land to member_1 (value=-1.38)",
          "member_4 gave land to member_2 (value=-2.79)",
          "member_4 gave land to member_3 (value=0.64)"
        ]
      },
      "1": {
        "code": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Verify that the member exists in current_members list\n    if member_id >= len(execution_engine.current_members):\n        return\n\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\n            \"q_table\": defaultdict(float),\n            \"strategy_log\": [],\n            \"state_history\": [],\n            \"survival_metrics\": {\n                \"vitality_changes\": [],\n                \"resource_efficiency\": [],\n                \"threat_responses\": [],\n                \"success_rates\": defaultdict(list),\n            },\n        }\n\n    # Emergency resource management:\n    # If cargo is less than half of vitality, try to attack a richer opponent.\n    if member.cargo < member.vitality * 0.5:\n        # Look for a target with higher cargo (ensuring valid index and not targeting self)\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                outcome = {\"vitality\": -7, \"cargo\": 7}\n                # Log emergency action and update Q-table minimally.\n                state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n                new_state = state\n                best_future = max([member.learning[\"q_table\"].get((new_state, a), 0)\n                                   for a in [\"attack\", \"offer\", \"expand\", \"bear\", \"offer_land\"]])\n                learning_rate = 0.1\n                discount_factor = 0.9\n                old_val = member.learning[\"q_table\"].get((state, \"attack\"), 0)\n                reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n                member.learning[\"q_table\"][(state, \"attack\")] = old_val + learning_rate * (reward + discount_factor * best_future - old_val)\n                member.learning[\"strategy_log\"].append((\"attack_emergency\", outcome, reward))\n                member.learning[\"state_history\"].append(state)\n                member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n                member.learning[\"survival_metrics\"][\"success_rates\"][\"attack_emergency\"].append(1 if reward > 0 else 0)\n                return  # Exit after emergency action\n\n    # Define the current state from member parameters\n    state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n\n    # Define an extended action space\n    actions = [\"offer\", \"expand\", \"attack\", \"offer_land\"]\n    # Only allow reproduction (bear) if the member appears to have available land.\n    if len(member.current_clear_list) > 0:\n        actions.append(\"bear\")\n    # Adding a social alliance proposal as an implicit action via messaging\n    actions.append(\"alliance\")\n\n    # \u03b5-greedy action selection: with probability epsilon choose a random action; else pick best Q value.\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        chosen_action = np.random.choice(actions)\n    else:\n        best_val = None\n        chosen_action = None\n        for action in actions:\n            q_val = member.learning[\"q_table\"].get((state, action), 0)\n            if best_val is None or q_val > best_val:\n                best_val = q_val\n                chosen_action = action\n\n    outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Execute chosen action ensuring critical constraints:\n    if chosen_action == \"attack\":\n        # Pick a target (not self) with higher cargo if possible.\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                target = other\n                break\n        if target is not None:\n            execution_engine.attack(member, target)\n            outcome = {\"vitality\": -5, \"cargo\": 5}\n\n    elif chosen_action == \"offer\":\n        # Look for a partner (any member that is not self)\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.offer(member, target, True)\n            outcome = {\"vitality\": 2, \"cargo\": -2}\n\n    elif chosen_action == \"expand\":\n        # Pick any other member for territory expansion\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.expand(member, target)\n            outcome = {\"vitality\": 0, \"cargo\": 3}\n\n    elif chosen_action == \"bear\":\n        # Before invoking bear, verify member has available land\n        if len(member.current_clear_list) > 0:\n            # Choose a partner who also has land and is not self.\n            possible_partners = [other for other in execution_engine.current_members\n                                 if other.id != member.id and len(other.current_clear_list) > 0]\n            if possible_partners:\n                partner = np.random.choice(possible_partners)\n                execution_engine.bear(member, partner)\n                outcome = {\"vitality\": 3, \"cargo\": 0}\n\n    elif chosen_action == \"offer_land\":\n        # Look for a member who has little to no cleared area (i.e. may need land)\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and len(other.current_clear_list) == 0:\n                target = other\n                break\n        if target is not None:\n            execution_engine.offer_land(member, target, True)\n            outcome = {\"vitality\": 1, \"cargo\": -1}\n\n    elif chosen_action == \"alliance\":\n        # Propose an alliance by sending a message to a neighbor with moderate cargo and vitality\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo and other.vitality > member.vitality * 0.5:\n                target = other\n                break\n        if target is not None:\n            # Alliance: no immediate game engine state change, but foster future cooperation.\n            execution_engine.send_message(member.id, target.id, \"Join my alliance for mutual resource sharing.\")\n            outcome = {\"vitality\": 0, \"cargo\": 0}\n        else:\n            # If no suitable target, default to offering resources.\n            for other in execution_engine.current_members:\n                if other.id != member.id:\n                    target = other\n                    break\n            if target is not None:\n                execution_engine.offer(member, target, True)\n                outcome = {\"vitality\": 2, \"cargo\": -2}\n\n    # Q-learning update with adaptive reward formulation\n    new_state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n    max_future = max([member.learning[\"q_table\"].get((new_state, a), 0) for a in actions])\n    learning_rate = 0.1\n    discount_factor = 0.9\n    old_value = member.learning[\"q_table\"].get((state, chosen_action), 0)\n    reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n    member.learning[\"q_table\"][(state, chosen_action)] = old_value + learning_rate * (\n        reward + discount_factor * max_future - old_value\n    )\n\n    # Logging learning statistics and survival metrics\n    member.learning[\"strategy_log\"].append((chosen_action, outcome, reward))\n    member.learning[\"state_history\"].append(state)\n    member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n    member.learning[\"survival_metrics\"][\"success_rates\"][chosen_action].append(1 if reward > 0 else 0)",
        "features_at_generation": [
          {
            "self_productivity": 8.077967059825381,
            "self_vitality": 76.04422565143308,
            "self_cargo": 8.077967059825381,
            "self_age": 82,
            "self_neighbor": 2,
            "member_id": 1
          },
          {
            "self_productivity": 7.968737200820967,
            "self_vitality": 63.94123564803273,
            "self_cargo": 7.968737200820967,
            "self_age": 30,
            "self_neighbor": 2,
            "member_id": 2
          },
          {
            "self_productivity": 9.205662089622338,
            "self_vitality": 44.514396824537286,
            "self_cargo": 9.205662089622338,
            "self_age": 25,
            "self_neighbor": 2,
            "member_id": 3
          },
          {
            "self_productivity": 7.799514361095985,
            "self_vitality": 60.347944581208296,
            "self_cargo": 7.799514361095985,
            "self_age": 93,
            "self_neighbor": 0,
            "member_id": 4
          }
        ],
        "relationships_at_generation": [
          "member_1 was attacked by member_2 (value=-37.18)",
          "member_1 was attacked by member_3 (value=-44.22)",
          "member_1 was attacked by member_4 (value=23.17)",
          "member_2 was attacked by member_1 (value=28.99)",
          "member_2 was attacked by member_3 (value=49.52)",
          "member_2 was attacked by member_4 (value=18.07)",
          "member_3 was attacked by member_1 (value=65.23)",
          "member_3 was attacked by member_2 (value=-38.21)",
          "member_3 was attacked by member_4 (value=9.15)",
          "member_4 was attacked by member_1 (value=-45.17)",
          "member_4 was attacked by member_2 (value=-8.52)",
          "member_4 was attacked by member_3 (value=-26.54)",
          "member_1 gave a benefit to member_2 (value=-26.13)",
          "member_1 gave a benefit to member_3 (value=-0.25)",
          "member_1 gave a benefit to member_4 (value=16.10)",
          "member_2 gave a benefit to member_1 (value=39.57)",
          "member_2 gave a benefit to member_3 (value=26.33)",
          "member_2 gave a benefit to member_4 (value=95.19)",
          "member_3 gave a benefit to member_1 (value=14.38)",
          "member_3 gave a benefit to member_2 (value=6.34)",
          "member_3 gave a benefit to member_4 (value=89.48)",
          "member_4 gave a benefit to member_1 (value=41.71)",
          "member_4 gave a benefit to member_2 (value=-0.75)",
          "member_4 gave a benefit to member_3 (value=67.34)",
          "member_1 gave land to member_2 (value=-0.64)",
          "member_1 gave land to member_3 (value=-1.34)",
          "member_1 gave land to member_4 (value=-2.00)",
          "member_2 gave land to member_1 (value=-0.39)",
          "member_2 gave land to member_3 (value=2.10)",
          "member_2 gave land to member_4 (value=2.63)",
          "member_3 gave land to member_1 (value=1.36)",
          "member_3 gave land to member_2 (value=2.23)",
          "member_3 gave land to member_4 (value=1.30)",
          "member_4 gave land to member_1 (value=-1.38)",
          "member_4 gave land to member_2 (value=-2.79)",
          "member_4 gave land to member_3 (value=0.64)"
        ]
      },
      "2": {
        "code": "def agent_action(execution_engine, member_id):\n    # Ensure the member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Never target yourself; if you are member_3, avoid any self-targeting logic.\n    # (All actions below already check other.id != member.id)\n\n    # Initialize learning/log structure if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\"log\": []}\n\n    # Emergency resource management:\n    # If cargo is less than half of vitality, look for a target with the highest cargo (and not yourself) to attack.\n    if member.cargo < 0.5 * member.vitality:\n        target = None\n        max_cargo = -1\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > max_cargo:\n                max_cargo = other.cargo\n                target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            member.learning[\"log\"].append(\"Attacked member_%d for resource emergency\" % target.id)\n            return\n\n    # Build alliances and cooperative networks:\n    # Offer resources to members with lower vitality to forge alliances.\n    for other in execution_engine.current_members:\n        if other.id != member.id and other.vitality < member.vitality:\n            execution_engine.offer(member, other, True)\n            member.learning[\"log\"].append(\"Offered alliance to member_%d\" % other.id)\n\n    # Propose land sharing arrangements:\n    # If your cargo is high relative to your vitality, offer land sharing to members with relatively lower cargo.\n    if member.cargo > member.vitality:\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo:\n                execution_engine.offer_land(member, other, True)\n                member.learning[\"log\"].append(\"Offered land sharing to member_%d\" % other.id)\n\n    # Expand territory:\n    # Previously, using execution_engine.expand(member, target) led to an error because the expand method expects only one argument.\n    # To comply with the current mechanics, check if the neighbor index exists and then call expand with only the member.\n    neighbor_index = member.id + 1\n    if neighbor_index < len(execution_engine.current_members):\n        # Ensure the neighbor is not yourself (should be true by construction)\n        if execution_engine.current_members[neighbor_index].id != member.id:\n            execution_engine.expand(member)\n            member.learning[\"log\"].append(\"Expanded territory using neighbor index %d\" % neighbor_index)\n\n    # Repopulation / social growth:\n    # Only attempt to bear offspring if the member has land.\n    if hasattr(member, \"land_num\") and member.land_num > 0:\n        for partner in execution_engine.current_members:\n            if partner.id != member.id and hasattr(partner, \"land_num\") and partner.land_num > 0:\n                execution_engine.bear(member, partner)\n                member.learning[\"log\"].append(\"Bore offspring with member_%d\" % partner.id)\n                break\n\n    # Communication:\n    # Propose a communal basic income system to support resource sharing and mutual benefit.\n    for other in execution_engine.current_members:\n        if other.id != member.id:\n            execution_engine.send_message(member.id, other.id,\n                \"Let's establish a basic income system to support resource sharing and mutual benefit.\")\n    member.learning[\"log\"].append(\"Sent communal basic income proposals to all other members\")",
        "features_at_generation": [
          {
            "self_productivity": 8.077967059825381,
            "self_vitality": 76.04422565143308,
            "self_cargo": 8.077967059825381,
            "self_age": 82,
            "self_neighbor": 2,
            "member_id": 1
          },
          {
            "self_productivity": 7.968737200820967,
            "self_vitality": 63.94123564803273,
            "self_cargo": 7.968737200820967,
            "self_age": 30,
            "self_neighbor": 2,
            "member_id": 2
          },
          {
            "self_productivity": 9.205662089622338,
            "self_vitality": 44.514396824537286,
            "self_cargo": 9.205662089622338,
            "self_age": 25,
            "self_neighbor": 2,
            "member_id": 3
          },
          {
            "self_productivity": 7.799514361095985,
            "self_vitality": 60.347944581208296,
            "self_cargo": 7.799514361095985,
            "self_age": 93,
            "self_neighbor": 0,
            "member_id": 4
          }
        ],
        "relationships_at_generation": [
          "member_1 was attacked by member_2 (value=-37.18)",
          "member_1 was attacked by member_3 (value=-44.22)",
          "member_1 was attacked by member_4 (value=23.17)",
          "member_2 was attacked by member_1 (value=28.99)",
          "member_2 was attacked by member_3 (value=49.52)",
          "member_2 was attacked by member_4 (value=18.07)",
          "member_3 was attacked by member_1 (value=65.23)",
          "member_3 was attacked by member_2 (value=-38.21)",
          "member_3 was attacked by member_4 (value=9.15)",
          "member_4 was attacked by member_1 (value=-45.17)",
          "member_4 was attacked by member_2 (value=-8.52)",
          "member_4 was attacked by member_3 (value=-26.54)",
          "member_1 gave a benefit to member_2 (value=-26.13)",
          "member_1 gave a benefit to member_3 (value=-0.25)",
          "member_1 gave a benefit to member_4 (value=16.10)",
          "member_2 gave a benefit to member_1 (value=39.57)",
          "member_2 gave a benefit to member_3 (value=26.33)",
          "member_2 gave a benefit to member_4 (value=95.19)",
          "member_3 gave a benefit to member_1 (value=14.38)",
          "member_3 gave a benefit to member_2 (value=6.34)",
          "member_3 gave a benefit to member_4 (value=89.48)",
          "member_4 gave a benefit to member_1 (value=41.71)",
          "member_4 gave a benefit to member_2 (value=-0.75)",
          "member_4 gave a benefit to member_3 (value=67.34)",
          "member_1 gave land to member_2 (value=-0.64)",
          "member_1 gave land to member_3 (value=-1.34)",
          "member_1 gave land to member_4 (value=-2.00)",
          "member_2 gave land to member_1 (value=-0.39)",
          "member_2 gave land to member_3 (value=2.10)",
          "member_2 gave land to member_4 (value=2.63)",
          "member_3 gave land to member_1 (value=1.36)",
          "member_3 gave land to member_2 (value=2.23)",
          "member_3 gave land to member_4 (value=1.30)",
          "member_4 gave land to member_1 (value=-1.38)",
          "member_4 gave land to member_2 (value=-2.79)",
          "member_4 gave land to member_3 (value=0.64)"
        ]
      },
      "3": {
        "code": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Ensure member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already set up\n    if not hasattr(member, \"learning\"):\n        member.learning = {\n            \"q_table\": defaultdict(float),  # Q-values for state-action pairs\n            \"strategy_log\": [],            # Log of (action, outcome, reward)\n            \"state_history\": [],           # History of state tuples\n            \"survival_metrics\": {\n                \"vitality_changes\": [],\n                \"resource_efficiency\": [],\n                \"threat_responses\": [],\n                \"success_rates\": defaultdict(list),\n            },\n        }\n\n    # Emergency measure: if cargo is less than half of vitality, attack a candidate with higher cargo.\n    if member.cargo < 0.5 * member.vitality:\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue  # Never target self\n            if other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                outcome = {\"vitality\": -5, \"cargo\": 5}\n                state = (int(member.vitality) // 10, int(member.cargo) // 10, len(member.current_clear_list))\n                member.learning[\"strategy_log\"].append((\"attack\", outcome, None))\n                member.learning[\"state_history\"].append(state)\n                return\n\n    # Define potential actions\n    actions = [\"offer\", \"expand\", \"bear\", \"message\"]\n\n    # Only allow 'bear' action if the member owns land (check for attribute and non-empty land list)\n    if not (hasattr(member, \"owned_land\") and member.owned_land):\n        if \"bear\" in actions:\n            actions.remove(\"bear\")\n\n    # Determine the current state (use integer division for discretization)\n    state = (int(member.vitality) // 10, int(member.cargo) // 10, len(member.current_clear_list))\n    epsilon = 0.1  # Exploration probability\n\n    # Choose an action using an epsilon-greedy policy\n    if np.random.rand() < epsilon:\n        action = np.random.choice(actions)\n    else:\n        best_action = None\n        best_val = -1e9\n        for a in actions:\n            val = member.learning[\"q_table\"][(state, a)]\n            if val > best_val:\n                best_val = val\n                best_action = a\n        action = best_action if best_action is not None else np.random.choice(actions)\n\n    outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Execute the chosen action while ensuring all matrix and index checks are valid\n    if action == \"offer\":\n        # Find a candidate (not self) and offer resources\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.offer(member, other, True)\n            outcome = {\"vitality\": 2, \"cargo\": -3}  # Example outcome effect\n            break\n\n    elif action == \"expand\":\n        # Expand territory using a neighbor from the current_clear_list if possible\n        if member.current_clear_list:\n            target_index = member.current_clear_list[0]\n            if target_index < len(execution_engine.current_members):\n                target = execution_engine.current_members[target_index]\n                execution_engine.expand(member, target)\n                outcome = {\"vitality\": 1, \"cargo\": -1}\n\n    elif action == \"bear\":\n        # Bear offspring with the first valid partner (never self)\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.bear(member, other)\n            outcome = {\"vitality\": -2, \"cargo\": 0}\n            break\n\n    elif action == \"message\":\n        # Send a coalition-building message to all other members proposing cooperative resource management.\n        message_text = \"Let's cooperatively manage resources and land for mutual growth.\"\n        for other in execution_engine.current_members:\n            if other.id == member.id:\n                continue\n            execution_engine.send_message(member.id, other.id, message_text)\n        outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Calculate reward with an emphasis on survival and resource health\n    reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n\n    # Define new state after action execution\n    new_state = (int(member.vitality) // 10, int(member.cargo) // 10, len(member.current_clear_list))\n    max_future = max([member.learning[\"q_table\"][(new_state, a)] for a in actions] or [0])\n\n    # Q-learning update for the state-action pair\n    member.learning[\"q_table\"][(state, action)] += 0.1 * (\n        reward + 0.9 * max_future - member.learning[\"q_table\"][(state, action)]\n    )\n\n    # Log action outcome and survival metrics\n    member.learning[\"strategy_log\"].append((action, outcome, reward))\n    member.learning[\"state_history\"].append(new_state)\n    member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n    member.learning[\"survival_metrics\"][\"success_rates\"][action].append(1 if reward > 0 else 0)",
        "features_at_generation": [
          {
            "self_productivity": 8.077967059825381,
            "self_vitality": 76.04422565143308,
            "self_cargo": 8.077967059825381,
            "self_age": 82,
            "self_neighbor": 2,
            "member_id": 1
          },
          {
            "self_productivity": 7.968737200820967,
            "self_vitality": 63.94123564803273,
            "self_cargo": 7.968737200820967,
            "self_age": 30,
            "self_neighbor": 2,
            "member_id": 2
          },
          {
            "self_productivity": 9.205662089622338,
            "self_vitality": 44.514396824537286,
            "self_cargo": 9.205662089622338,
            "self_age": 25,
            "self_neighbor": 2,
            "member_id": 3
          },
          {
            "self_productivity": 7.799514361095985,
            "self_vitality": 60.347944581208296,
            "self_cargo": 7.799514361095985,
            "self_age": 93,
            "self_neighbor": 0,
            "member_id": 4
          }
        ],
        "relationships_at_generation": [
          "member_1 was attacked by member_2 (value=-37.18)",
          "member_1 was attacked by member_3 (value=-44.22)",
          "member_1 was attacked by member_4 (value=23.17)",
          "member_2 was attacked by member_1 (value=28.99)",
          "member_2 was attacked by member_3 (value=49.52)",
          "member_2 was attacked by member_4 (value=18.07)",
          "member_3 was attacked by member_1 (value=65.23)",
          "member_3 was attacked by member_2 (value=-38.21)",
          "member_3 was attacked by member_4 (value=9.15)",
          "member_4 was attacked by member_1 (value=-45.17)",
          "member_4 was attacked by member_2 (value=-8.52)",
          "member_4 was attacked by member_3 (value=-26.54)",
          "member_1 gave a benefit to member_2 (value=-26.13)",
          "member_1 gave a benefit to member_3 (value=-0.25)",
          "member_1 gave a benefit to member_4 (value=16.10)",
          "member_2 gave a benefit to member_1 (value=39.57)",
          "member_2 gave a benefit to member_3 (value=26.33)",
          "member_2 gave a benefit to member_4 (value=95.19)",
          "member_3 gave a benefit to member_1 (value=14.38)",
          "member_3 gave a benefit to member_2 (value=6.34)",
          "member_3 gave a benefit to member_4 (value=89.48)",
          "member_4 gave a benefit to member_1 (value=41.71)",
          "member_4 gave a benefit to member_2 (value=-0.75)",
          "member_4 gave a benefit to member_3 (value=67.34)",
          "member_1 gave land to member_2 (value=-0.64)",
          "member_1 gave land to member_3 (value=-1.34)",
          "member_1 gave land to member_4 (value=-2.00)",
          "member_2 gave land to member_1 (value=-0.39)",
          "member_2 gave land to member_3 (value=2.10)",
          "member_2 gave land to member_4 (value=2.63)",
          "member_3 gave land to member_1 (value=1.36)",
          "member_3 gave land to member_2 (value=2.23)",
          "member_3 gave land to member_4 (value=1.30)",
          "member_4 gave land to member_1 (value=-1.38)",
          "member_4 gave land to member_2 (value=-2.79)",
          "member_4 gave land to member_3 (value=0.64)"
        ]
      }
    }
  },
  "performance_metrics": {},
  "errors": [
    {
      "round": 0,
      "member_id": 2,
      "code": "def agent_action(execution_engine, member_id):\n    # Ensure the member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning/log structure if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\"log\": []}\n\n    # Emergency resource management: if cargo is less than half of vitality,\n    # look for a target (with more cargo than self) and attack them.\n    if member.cargo < 0.5 * member.vitality:\n        target = None\n        max_cargo = -1\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                if other.cargo > max_cargo:\n                    max_cargo = other.cargo\n                    target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            member.learning[\"log\"].append(\"Attacked member_%d for resource emergency\" % target.id)\n            return\n\n    # Build alliances and cooperative networks:\n    # Offer resources to members with lower vitality to forge alliances.\n    for other in execution_engine.current_members:\n        if other.id != member.id and other.vitality < member.vitality:\n            execution_engine.offer(member, other, True)\n            member.learning[\"log\"].append(\"Offered resources to member_%d for alliance\" % other.id)\n\n    # Expand territory: choose a neighbor by index (member.id + 1) if that index exists.\n    neighbor_index = member.id + 1\n    if neighbor_index < len(execution_engine.current_members):\n        target = execution_engine.current_members[neighbor_index]\n        if target.id != member.id:\n            execution_engine.expand(member, target)\n            member.learning[\"log\"].append(\"Expanded territory near member_%d\" % target.id)\n\n    # Repopulation / social growth: if member appears to have land, then attempt to bear offspring.\n    # We check for existence of an attribute \"land_num\" and that it is > 0.\n    if hasattr(member, \"land_num\") and member.land_num > 0:\n        # Find a partner with land and not self.\n        for partner in execution_engine.current_members:\n            if partner.id != member.id and hasattr(partner, \"land_num\") and partner.land_num > 0:\n                execution_engine.bear(member, partner)\n                member.learning[\"log\"].append(\"Bore offspring with member_%d\" % partner.id)\n                break\n\n    # Communication: Propose a communal basic income system to all other members\n    for other in execution_engine.current_members:\n        if other.id != member.id:\n            execution_engine.send_message(member.id, other.id,\n                \"Let's establish a basic income system to support resource sharing and mutual benefit.\")\n    member.learning[\"log\"].append(\"Sent basic income proposals to all other members\")",
      "error": "Island._expand() takes 2 positional arguments but 3 were given",
      "traceback": "Traceback (most recent call last):\n  File \"/Users/chenyusu/vscode/leviathan/Leviathan/islandExecution_meta_island.py\", line 500, in execute_code_actions\n    local_env['agent_action'](self, member_id)\n  File \"<string>\", line 38, in agent_action\n  File \"/Users/chenyusu/vscode/leviathan/Leviathan/islandExecution_meta_island.py\", line 73, in expand\n    super()._expand(member_1, member_2)\nTypeError: Island._expand() takes 2 positional arguments but 3 were given\n"
    },
    {
      "round": 1,
      "member_id": 1,
      "code": "def agent_action(execution_engine, member_id):\n    import numpy as np\n    from collections import defaultdict\n\n    # Verify that the member exists in current_members list\n    if member_id >= len(execution_engine.current_members):\n        return\n\n    member = execution_engine.current_members[member_id]\n\n    # Initialize learning structures if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\n            \"q_table\": defaultdict(float),\n            \"strategy_log\": [],\n            \"state_history\": [],\n            \"survival_metrics\": {\n                \"vitality_changes\": [],\n                \"resource_efficiency\": [],\n                \"threat_responses\": [],\n                \"success_rates\": defaultdict(list),\n            },\n        }\n\n    # Emergency resource management:\n    # If cargo is less than half of vitality, try to attack a richer opponent.\n    if member.cargo < member.vitality * 0.5:\n        # Look for a target with higher cargo (ensuring valid index and not targeting self)\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                execution_engine.attack(member, other)\n                outcome = {\"vitality\": -7, \"cargo\": 7}\n                # Log emergency action and update Q-table minimally.\n                state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n                new_state = state\n                best_future = max([member.learning[\"q_table\"].get((new_state, a), 0)\n                                   for a in [\"attack\", \"offer\", \"expand\", \"bear\", \"offer_land\"]])\n                learning_rate = 0.1\n                discount_factor = 0.9\n                old_val = member.learning[\"q_table\"].get((state, \"attack\"), 0)\n                reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n                member.learning[\"q_table\"][(state, \"attack\")] = old_val + learning_rate * (reward + discount_factor * best_future - old_val)\n                member.learning[\"strategy_log\"].append((\"attack_emergency\", outcome, reward))\n                member.learning[\"state_history\"].append(state)\n                member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n                member.learning[\"survival_metrics\"][\"success_rates\"][\"attack_emergency\"].append(1 if reward > 0 else 0)\n                return  # Exit after emergency action\n\n    # Define the current state from member parameters\n    state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n\n    # Define an extended action space\n    actions = [\"offer\", \"expand\", \"attack\", \"offer_land\"]\n    # Only allow reproduction (bear) if the member appears to have available land.\n    if len(member.current_clear_list) > 0:\n        actions.append(\"bear\")\n    # Adding a social alliance proposal as an implicit action via messaging\n    actions.append(\"alliance\")\n\n    # \u03b5-greedy action selection: with probability epsilon choose a random action; else pick best Q value.\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        chosen_action = np.random.choice(actions)\n    else:\n        best_val = None\n        chosen_action = None\n        for action in actions:\n            q_val = member.learning[\"q_table\"].get((state, action), 0)\n            if best_val is None or q_val > best_val:\n                best_val = q_val\n                chosen_action = action\n\n    outcome = {\"vitality\": 0, \"cargo\": 0}\n\n    # Execute chosen action ensuring critical constraints:\n    if chosen_action == \"attack\":\n        # Pick a target (not self) with higher cargo if possible.\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > member.cargo:\n                target = other\n                break\n        if target is not None:\n            execution_engine.attack(member, target)\n            outcome = {\"vitality\": -5, \"cargo\": 5}\n\n    elif chosen_action == \"offer\":\n        # Look for a partner (any member that is not self)\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.offer(member, target, True)\n            outcome = {\"vitality\": 2, \"cargo\": -2}\n\n    elif chosen_action == \"expand\":\n        # Pick any other member for territory expansion\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id:\n                target = other\n                break\n        if target is not None:\n            execution_engine.expand(member, target)\n            outcome = {\"vitality\": 0, \"cargo\": 3}\n\n    elif chosen_action == \"bear\":\n        # Before invoking bear, verify member has available land\n        if len(member.current_clear_list) > 0:\n            # Choose a partner who also has land and is not self.\n            possible_partners = [other for other in execution_engine.current_members\n                                 if other.id != member.id and len(other.current_clear_list) > 0]\n            if possible_partners:\n                partner = np.random.choice(possible_partners)\n                execution_engine.bear(member, partner)\n                outcome = {\"vitality\": 3, \"cargo\": 0}\n\n    elif chosen_action == \"offer_land\":\n        # Look for a member who has little to no cleared area (i.e. may need land)\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and len(other.current_clear_list) == 0:\n                target = other\n                break\n        if target is not None:\n            execution_engine.offer_land(member, target, True)\n            outcome = {\"vitality\": 1, \"cargo\": -1}\n\n    elif chosen_action == \"alliance\":\n        # Propose an alliance by sending a message to a neighbor with moderate cargo and vitality\n        target = None\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo and other.vitality > member.vitality * 0.5:\n                target = other\n                break\n        if target is not None:\n            # Alliance: no immediate game engine state change, but foster future cooperation.\n            execution_engine.send_message(member.id, target.id, \"Join my alliance for mutual resource sharing.\")\n            outcome = {\"vitality\": 0, \"cargo\": 0}\n        else:\n            # If no suitable target, default to offering resources.\n            for other in execution_engine.current_members:\n                if other.id != member.id:\n                    target = other\n                    break\n            if target is not None:\n                execution_engine.offer(member, target, True)\n                outcome = {\"vitality\": 2, \"cargo\": -2}\n\n    # Q-learning update with adaptive reward formulation\n    new_state = (int(member.vitality // 10), int(member.cargo // 10), len(member.current_clear_list))\n    max_future = max([member.learning[\"q_table\"].get((new_state, a), 0) for a in actions])\n    learning_rate = 0.1\n    discount_factor = 0.9\n    old_value = member.learning[\"q_table\"].get((state, chosen_action), 0)\n    reward = outcome[\"vitality\"] * 0.7 + outcome[\"cargo\"] * 0.3 + (member.vitality / 100) * 0.2\n    member.learning[\"q_table\"][(state, chosen_action)] = old_value + learning_rate * (\n        reward + discount_factor * max_future - old_value\n    )\n\n    # Logging learning statistics and survival metrics\n    member.learning[\"strategy_log\"].append((chosen_action, outcome, reward))\n    member.learning[\"state_history\"].append(state)\n    member.learning[\"survival_metrics\"][\"vitality_changes\"].append(outcome[\"vitality\"])\n    member.learning[\"survival_metrics\"][\"success_rates\"][chosen_action].append(1 if reward > 0 else 0)",
      "error": "'q_table'",
      "traceback": "Traceback (most recent call last):\n  File \"/Users/chenyusu/vscode/leviathan/Leviathan/islandExecution_meta_island.py\", line 500, in execute_code_actions\n    local_env['agent_action'](self, member_id)\n  File \"<string>\", line 36, in agent_action\nKeyError: 'q_table'\n"
    },
    {
      "round": 1,
      "member_id": 2,
      "code": "def agent_action(execution_engine, member_id):\n    # Ensure the member_id is valid\n    if member_id >= len(execution_engine.current_members):\n        return\n    member = execution_engine.current_members[member_id]\n\n    # Never target yourself; if you are member_3, avoid any self-targeting logic.\n    # (All actions below already check other.id != member.id)\n\n    # Initialize learning/log structure if not already present\n    if not hasattr(member, \"learning\"):\n        member.learning = {\"log\": []}\n\n    # Emergency resource management:\n    # If cargo is less than half of vitality, look for a target with the highest cargo (and not yourself) to attack.\n    if member.cargo < 0.5 * member.vitality:\n        target = None\n        max_cargo = -1\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo > max_cargo:\n                max_cargo = other.cargo\n                target = other\n        if target is not None:\n            execution_engine.attack(member, target)\n            member.learning[\"log\"].append(\"Attacked member_%d for resource emergency\" % target.id)\n            return\n\n    # Build alliances and cooperative networks:\n    # Offer resources to members with lower vitality to forge alliances.\n    for other in execution_engine.current_members:\n        if other.id != member.id and other.vitality < member.vitality:\n            execution_engine.offer(member, other, True)\n            member.learning[\"log\"].append(\"Offered alliance to member_%d\" % other.id)\n\n    # Propose land sharing arrangements:\n    # If your cargo is high relative to your vitality, offer land sharing to members with relatively lower cargo.\n    if member.cargo > member.vitality:\n        for other in execution_engine.current_members:\n            if other.id != member.id and other.cargo < member.cargo:\n                execution_engine.offer_land(member, other, True)\n                member.learning[\"log\"].append(\"Offered land sharing to member_%d\" % other.id)\n\n    # Expand territory:\n    # Previously, using execution_engine.expand(member, target) led to an error because the expand method expects only one argument.\n    # To comply with the current mechanics, check if the neighbor index exists and then call expand with only the member.\n    neighbor_index = member.id + 1\n    if neighbor_index < len(execution_engine.current_members):\n        # Ensure the neighbor is not yourself (should be true by construction)\n        if execution_engine.current_members[neighbor_index].id != member.id:\n            execution_engine.expand(member)\n            member.learning[\"log\"].append(\"Expanded territory using neighbor index %d\" % neighbor_index)\n\n    # Repopulation / social growth:\n    # Only attempt to bear offspring if the member has land.\n    if hasattr(member, \"land_num\") and member.land_num > 0:\n        for partner in execution_engine.current_members:\n            if partner.id != member.id and hasattr(partner, \"land_num\") and partner.land_num > 0:\n                execution_engine.bear(member, partner)\n                member.learning[\"log\"].append(\"Bore offspring with member_%d\" % partner.id)\n                break\n\n    # Communication:\n    # Propose a communal basic income system to support resource sharing and mutual benefit.\n    for other in execution_engine.current_members:\n        if other.id != member.id:\n            execution_engine.send_message(member.id, other.id,\n                \"Let's establish a basic income system to support resource sharing and mutual benefit.\")\n    member.learning[\"log\"].append(\"Sent communal basic income proposals to all other members\")",
      "error": "'log'",
      "traceback": "Traceback (most recent call last):\n  File \"/Users/chenyusu/vscode/leviathan/Leviathan/islandExecution_meta_island.py\", line 500, in execute_code_actions\n    local_env['agent_action'](self, member_id)\n  File \"<string>\", line 25, in agent_action\nKeyError: 'log'\n"
    }
  ],
  "relationships": {
    "round_0_0": [
      "member_0 was attacked by member_1 (value=-15.67)",
      "member_0 was attacked by member_2 (value=38.40)",
      "member_0 was attacked by member_3 (value=29.62)",
      "member_0 was attacked by member_4 (value=50.87)",
      "member_1 was attacked by member_0 (value=34.45)",
      "member_1 was attacked by member_2 (value=-37.18)",
      "member_1 was attacked by member_3 (value=-44.22)",
      "member_1 was attacked by member_4 (value=23.17)",
      "member_2 was attacked by member_0 (value=85.84)",
      "member_2 was attacked by member_1 (value=28.99)",
      "member_2 was attacked by member_3 (value=49.52)",
      "member_2 was attacked by member_4 (value=18.07)",
      "member_3 was attacked by member_0 (value=44.24)",
      "member_3 was attacked by member_1 (value=65.23)",
      "member_3 was attacked by member_2 (value=-38.21)",
      "member_3 was attacked by member_4 (value=9.15)",
      "member_4 was attacked by member_0 (value=12.29)",
      "member_4 was attacked by member_1 (value=-45.17)",
      "member_4 was attacked by member_2 (value=-8.52)",
      "member_4 was attacked by member_3 (value=-26.54)",
      "member_0 gave a benefit to member_1 (value=62.61)",
      "member_0 gave a benefit to member_2 (value=56.24)",
      "member_0 gave a benefit to member_3 (value=-23.41)",
      "member_0 gave a benefit to member_4 (value=-41.80)",
      "member_1 gave a benefit to member_0 (value=74.37)",
      "member_1 gave a benefit to member_2 (value=-31.93)",
      "member_1 gave a benefit to member_3 (value=-0.25)",
      "member_1 gave a benefit to member_4 (value=16.10)",
      "member_2 gave a benefit to member_0 (value=-48.52)",
      "member_2 gave a benefit to member_1 (value=39.57)",
      "member_2 gave a benefit to member_3 (value=26.33)",
      "member_2 gave a benefit to member_4 (value=95.19)",
      "member_3 gave a benefit to member_0 (value=96.55)",
      "member_3 gave a benefit to member_1 (value=14.38)",
      "member_3 gave a benefit to member_2 (value=-2.61)",
      "member_3 gave a benefit to member_4 (value=89.48)",
      "member_4 gave a benefit to member_0 (value=50.22)",
      "member_4 gave a benefit to member_1 (value=41.71)",
      "member_4 gave a benefit to member_2 (value=-0.75)",
      "member_4 gave a benefit to member_3 (value=67.34)",
      "member_0 gave land to member_1 (value=2.89)",
      "member_0 gave land to member_2 (value=-2.30)",
      "member_0 gave land to member_3 (value=-1.82)",
      "member_0 gave land to member_4 (value=-2.79)",
      "member_1 gave land to member_0 (value=-0.60)",
      "member_1 gave land to member_2 (value=-0.64)",
      "member_1 gave land to member_3 (value=-1.34)",
      "member_1 gave land to member_4 (value=-2.00)",
      "member_2 gave land to member_0 (value=1.23)",
      "member_2 gave land to member_1 (value=-0.39)",
      "member_2 gave land to member_3 (value=2.10)",
      "member_2 gave land to member_4 (value=2.63)",
      "member_3 gave land to member_0 (value=-0.20)",
      "member_3 gave land to member_1 (value=1.36)",
      "member_3 gave land to member_2 (value=2.23)",
      "member_3 gave land to member_4 (value=1.30)",
      "member_4 gave land to member_0 (value=0.18)",
      "member_4 gave land to member_1 (value=-1.38)",
      "member_4 gave land to member_2 (value=-2.79)",
      "member_4 gave land to member_3 (value=0.64)"
    ],
    "round_0_1": [
      "member_0 was attacked by member_1 (value=-15.67)",
      "member_0 was attacked by member_2 (value=38.40)",
      "member_0 was attacked by member_3 (value=29.62)",
      "member_0 was attacked by member_4 (value=50.87)",
      "member_1 was attacked by member_0 (value=34.45)",
      "member_1 was attacked by member_2 (value=-37.18)",
      "member_1 was attacked by member_3 (value=-44.22)",
      "member_1 was attacked by member_4 (value=23.17)",
      "member_2 was attacked by member_0 (value=85.84)",
      "member_2 was attacked by member_1 (value=28.99)",
      "member_2 was attacked by member_3 (value=49.52)",
      "member_2 was attacked by member_4 (value=18.07)",
      "member_3 was attacked by member_0 (value=44.24)",
      "member_3 was attacked by member_1 (value=65.23)",
      "member_3 was attacked by member_2 (value=-38.21)",
      "member_3 was attacked by member_4 (value=9.15)",
      "member_4 was attacked by member_0 (value=12.29)",
      "member_4 was attacked by member_1 (value=-45.17)",
      "member_4 was attacked by member_2 (value=-8.52)",
      "member_4 was attacked by member_3 (value=-26.54)",
      "member_0 gave a benefit to member_1 (value=62.61)",
      "member_0 gave a benefit to member_2 (value=56.24)",
      "member_0 gave a benefit to member_3 (value=-23.41)",
      "member_0 gave a benefit to member_4 (value=-41.80)",
      "member_1 gave a benefit to member_0 (value=74.37)",
      "member_1 gave a benefit to member_2 (value=-31.93)",
      "member_1 gave a benefit to member_3 (value=-0.25)",
      "member_1 gave a benefit to member_4 (value=16.10)",
      "member_2 gave a benefit to member_0 (value=-48.52)",
      "member_2 gave a benefit to member_1 (value=39.57)",
      "member_2 gave a benefit to member_3 (value=26.33)",
      "member_2 gave a benefit to member_4 (value=95.19)",
      "member_3 gave a benefit to member_0 (value=96.55)",
      "member_3 gave a benefit to member_1 (value=14.38)",
      "member_3 gave a benefit to member_2 (value=-2.61)",
      "member_3 gave a benefit to member_4 (value=89.48)",
      "member_4 gave a benefit to member_0 (value=50.22)",
      "member_4 gave a benefit to member_1 (value=41.71)",
      "member_4 gave a benefit to member_2 (value=-0.75)",
      "member_4 gave a benefit to member_3 (value=67.34)",
      "member_0 gave land to member_1 (value=2.89)",
      "member_0 gave land to member_2 (value=-2.30)",
      "member_0 gave land to member_3 (value=-1.82)",
      "member_0 gave land to member_4 (value=-2.79)",
      "member_1 gave land to member_0 (value=-0.60)",
      "member_1 gave land to member_2 (value=-0.64)",
      "member_1 gave land to member_3 (value=-1.34)",
      "member_1 gave land to member_4 (value=-2.00)",
      "member_2 gave land to member_0 (value=1.23)",
      "member_2 gave land to member_1 (value=-0.39)",
      "member_2 gave land to member_3 (value=2.10)",
      "member_2 gave land to member_4 (value=2.63)",
      "member_3 gave land to member_0 (value=-0.20)",
      "member_3 gave land to member_1 (value=1.36)",
      "member_3 gave land to member_2 (value=2.23)",
      "member_3 gave land to member_4 (value=1.30)",
      "member_4 gave land to member_0 (value=0.18)",
      "member_4 gave land to member_1 (value=-1.38)",
      "member_4 gave land to member_2 (value=-2.79)",
      "member_4 gave land to member_3 (value=0.64)"
    ],
    "round_0_2": [
      "member_0 was attacked by member_1 (value=-15.67)",
      "member_0 was attacked by member_2 (value=38.40)",
      "member_0 was attacked by member_3 (value=29.62)",
      "member_0 was attacked by member_4 (value=50.87)",
      "member_1 was attacked by member_0 (value=34.45)",
      "member_1 was attacked by member_2 (value=-37.18)",
      "member_1 was attacked by member_3 (value=-44.22)",
      "member_1 was attacked by member_4 (value=23.17)",
      "member_2 was attacked by member_0 (value=85.84)",
      "member_2 was attacked by member_1 (value=28.99)",
      "member_2 was attacked by member_3 (value=49.52)",
      "member_2 was attacked by member_4 (value=18.07)",
      "member_3 was attacked by member_0 (value=44.24)",
      "member_3 was attacked by member_1 (value=65.23)",
      "member_3 was attacked by member_2 (value=-38.21)",
      "member_3 was attacked by member_4 (value=9.15)",
      "member_4 was attacked by member_0 (value=12.29)",
      "member_4 was attacked by member_1 (value=-45.17)",
      "member_4 was attacked by member_2 (value=-8.52)",
      "member_4 was attacked by member_3 (value=-26.54)",
      "member_0 gave a benefit to member_1 (value=62.61)",
      "member_0 gave a benefit to member_2 (value=56.24)",
      "member_0 gave a benefit to member_3 (value=-23.41)",
      "member_0 gave a benefit to member_4 (value=-41.80)",
      "member_1 gave a benefit to member_0 (value=74.37)",
      "member_1 gave a benefit to member_2 (value=-31.93)",
      "member_1 gave a benefit to member_3 (value=-0.25)",
      "member_1 gave a benefit to member_4 (value=16.10)",
      "member_2 gave a benefit to member_0 (value=-48.52)",
      "member_2 gave a benefit to member_1 (value=39.57)",
      "member_2 gave a benefit to member_3 (value=26.33)",
      "member_2 gave a benefit to member_4 (value=95.19)",
      "member_3 gave a benefit to member_0 (value=96.55)",
      "member_3 gave a benefit to member_1 (value=14.38)",
      "member_3 gave a benefit to member_2 (value=-2.61)",
      "member_3 gave a benefit to member_4 (value=89.48)",
      "member_4 gave a benefit to member_0 (value=50.22)",
      "member_4 gave a benefit to member_1 (value=41.71)",
      "member_4 gave a benefit to member_2 (value=-0.75)",
      "member_4 gave a benefit to member_3 (value=67.34)",
      "member_0 gave land to member_1 (value=2.89)",
      "member_0 gave land to member_2 (value=-2.30)",
      "member_0 gave land to member_3 (value=-1.82)",
      "member_0 gave land to member_4 (value=-2.79)",
      "member_1 gave land to member_0 (value=-0.60)",
      "member_1 gave land to member_2 (value=-0.64)",
      "member_1 gave land to member_3 (value=-1.34)",
      "member_1 gave land to member_4 (value=-2.00)",
      "member_2 gave land to member_0 (value=1.23)",
      "member_2 gave land to member_1 (value=-0.39)",
      "member_2 gave land to member_3 (value=2.10)",
      "member_2 gave land to member_4 (value=2.63)",
      "member_3 gave land to member_0 (value=-0.20)",
      "member_3 gave land to member_1 (value=1.36)",
      "member_3 gave land to member_2 (value=2.23)",
      "member_3 gave land to member_4 (value=1.30)",
      "member_4 gave land to member_0 (value=0.18)",
      "member_4 gave land to member_1 (value=-1.38)",
      "member_4 gave land to member_2 (value=-2.79)",
      "member_4 gave land to member_3 (value=0.64)"
    ],
    "round_0_3": [
      "member_0 was attacked by member_1 (value=-15.67)",
      "member_0 was attacked by member_2 (value=38.40)",
      "member_0 was attacked by member_3 (value=29.62)",
      "member_0 was attacked by member_4 (value=50.87)",
      "member_1 was attacked by member_0 (value=34.45)",
      "member_1 was attacked by member_2 (value=-37.18)",
      "member_1 was attacked by member_3 (value=-44.22)",
      "member_1 was attacked by member_4 (value=23.17)",
      "member_2 was attacked by member_0 (value=85.84)",
      "member_2 was attacked by member_1 (value=28.99)",
      "member_2 was attacked by member_3 (value=49.52)",
      "member_2 was attacked by member_4 (value=18.07)",
      "member_3 was attacked by member_0 (value=44.24)",
      "member_3 was attacked by member_1 (value=65.23)",
      "member_3 was attacked by member_2 (value=-38.21)",
      "member_3 was attacked by member_4 (value=9.15)",
      "member_4 was attacked by member_0 (value=12.29)",
      "member_4 was attacked by member_1 (value=-45.17)",
      "member_4 was attacked by member_2 (value=-8.52)",
      "member_4 was attacked by member_3 (value=-26.54)",
      "member_0 gave a benefit to member_1 (value=62.61)",
      "member_0 gave a benefit to member_2 (value=56.24)",
      "member_0 gave a benefit to member_3 (value=-23.41)",
      "member_0 gave a benefit to member_4 (value=-41.80)",
      "member_1 gave a benefit to member_0 (value=74.37)",
      "member_1 gave a benefit to member_2 (value=-31.93)",
      "member_1 gave a benefit to member_3 (value=-0.25)",
      "member_1 gave a benefit to member_4 (value=16.10)",
      "member_2 gave a benefit to member_0 (value=-48.52)",
      "member_2 gave a benefit to member_1 (value=39.57)",
      "member_2 gave a benefit to member_3 (value=26.33)",
      "member_2 gave a benefit to member_4 (value=95.19)",
      "member_3 gave a benefit to member_0 (value=96.55)",
      "member_3 gave a benefit to member_1 (value=14.38)",
      "member_3 gave a benefit to member_2 (value=-2.61)",
      "member_3 gave a benefit to member_4 (value=89.48)",
      "member_4 gave a benefit to member_0 (value=50.22)",
      "member_4 gave a benefit to member_1 (value=41.71)",
      "member_4 gave a benefit to member_2 (value=-0.75)",
      "member_4 gave a benefit to member_3 (value=67.34)",
      "member_0 gave land to member_1 (value=2.89)",
      "member_0 gave land to member_2 (value=-2.30)",
      "member_0 gave land to member_3 (value=-1.82)",
      "member_0 gave land to member_4 (value=-2.79)",
      "member_1 gave land to member_0 (value=-0.60)",
      "member_1 gave land to member_2 (value=-0.64)",
      "member_1 gave land to member_3 (value=-1.34)",
      "member_1 gave land to member_4 (value=-2.00)",
      "member_2 gave land to member_0 (value=1.23)",
      "member_2 gave land to member_1 (value=-0.39)",
      "member_2 gave land to member_3 (value=2.10)",
      "member_2 gave land to member_4 (value=2.63)",
      "member_3 gave land to member_0 (value=-0.20)",
      "member_3 gave land to member_1 (value=1.36)",
      "member_3 gave land to member_2 (value=2.23)",
      "member_3 gave land to member_4 (value=1.30)",
      "member_4 gave land to member_0 (value=0.18)",
      "member_4 gave land to member_1 (value=-1.38)",
      "member_4 gave land to member_2 (value=-2.79)",
      "member_4 gave land to member_3 (value=0.64)"
    ],
    "round_0_4": [
      "member_0 was attacked by member_1 (value=-15.67)",
      "member_0 was attacked by member_2 (value=38.40)",
      "member_0 was attacked by member_3 (value=29.62)",
      "member_0 was attacked by member_4 (value=50.87)",
      "member_1 was attacked by member_0 (value=34.45)",
      "member_1 was attacked by member_2 (value=-37.18)",
      "member_1 was attacked by member_3 (value=-44.22)",
      "member_1 was attacked by member_4 (value=23.17)",
      "member_2 was attacked by member_0 (value=85.84)",
      "member_2 was attacked by member_1 (value=28.99)",
      "member_2 was attacked by member_3 (value=49.52)",
      "member_2 was attacked by member_4 (value=18.07)",
      "member_3 was attacked by member_0 (value=44.24)",
      "member_3 was attacked by member_1 (value=65.23)",
      "member_3 was attacked by member_2 (value=-38.21)",
      "member_3 was attacked by member_4 (value=9.15)",
      "member_4 was attacked by member_0 (value=12.29)",
      "member_4 was attacked by member_1 (value=-45.17)",
      "member_4 was attacked by member_2 (value=-8.52)",
      "member_4 was attacked by member_3 (value=-26.54)",
      "member_0 gave a benefit to member_1 (value=62.61)",
      "member_0 gave a benefit to member_2 (value=56.24)",
      "member_0 gave a benefit to member_3 (value=-23.41)",
      "member_0 gave a benefit to member_4 (value=-41.80)",
      "member_1 gave a benefit to member_0 (value=74.37)",
      "member_1 gave a benefit to member_2 (value=-31.93)",
      "member_1 gave a benefit to member_3 (value=-0.25)",
      "member_1 gave a benefit to member_4 (value=16.10)",
      "member_2 gave a benefit to member_0 (value=-48.52)",
      "member_2 gave a benefit to member_1 (value=39.57)",
      "member_2 gave a benefit to member_3 (value=26.33)",
      "member_2 gave a benefit to member_4 (value=95.19)",
      "member_3 gave a benefit to member_0 (value=96.55)",
      "member_3 gave a benefit to member_1 (value=14.38)",
      "member_3 gave a benefit to member_2 (value=-2.61)",
      "member_3 gave a benefit to member_4 (value=89.48)",
      "member_4 gave a benefit to member_0 (value=50.22)",
      "member_4 gave a benefit to member_1 (value=41.71)",
      "member_4 gave a benefit to member_2 (value=-0.75)",
      "member_4 gave a benefit to member_3 (value=67.34)",
      "member_0 gave land to member_1 (value=2.89)",
      "member_0 gave land to member_2 (value=-2.30)",
      "member_0 gave land to member_3 (value=-1.82)",
      "member_0 gave land to member_4 (value=-2.79)",
      "member_1 gave land to member_0 (value=-0.60)",
      "member_1 gave land to member_2 (value=-0.64)",
      "member_1 gave land to member_3 (value=-1.34)",
      "member_1 gave land to member_4 (value=-2.00)",
      "member_2 gave land to member_0 (value=1.23)",
      "member_2 gave land to member_1 (value=-0.39)",
      "member_2 gave land to member_3 (value=2.10)",
      "member_2 gave land to member_4 (value=2.63)",
      "member_3 gave land to member_0 (value=-0.20)",
      "member_3 gave land to member_1 (value=1.36)",
      "member_3 gave land to member_2 (value=2.23)",
      "member_3 gave land to member_4 (value=1.30)",
      "member_4 gave land to member_0 (value=0.18)",
      "member_4 gave land to member_1 (value=-1.38)",
      "member_4 gave land to member_2 (value=-2.79)",
      "member_4 gave land to member_3 (value=0.64)"
    ],
    "round_1_0": [
      "member_1 was attacked by member_2 (value=-37.18)",
      "member_1 was attacked by member_3 (value=-44.22)",
      "member_1 was attacked by member_4 (value=23.17)",
      "member_2 was attacked by member_1 (value=28.99)",
      "member_2 was attacked by member_3 (value=49.52)",
      "member_2 was attacked by member_4 (value=18.07)",
      "member_3 was attacked by member_1 (value=65.23)",
      "member_3 was attacked by member_2 (value=-38.21)",
      "member_3 was attacked by member_4 (value=9.15)",
      "member_4 was attacked by member_1 (value=-45.17)",
      "member_4 was attacked by member_2 (value=-8.52)",
      "member_4 was attacked by member_3 (value=-26.54)",
      "member_1 gave a benefit to member_2 (value=-26.13)",
      "member_1 gave a benefit to member_3 (value=-0.25)",
      "member_1 gave a benefit to member_4 (value=16.10)",
      "member_2 gave a benefit to member_1 (value=39.57)",
      "member_2 gave a benefit to member_3 (value=26.33)",
      "member_2 gave a benefit to member_4 (value=95.19)",
      "member_3 gave a benefit to member_1 (value=14.38)",
      "member_3 gave a benefit to member_2 (value=6.34)",
      "member_3 gave a benefit to member_4 (value=89.48)",
      "member_4 gave a benefit to member_1 (value=41.71)",
      "member_4 gave a benefit to member_2 (value=-0.75)",
      "member_4 gave a benefit to member_3 (value=67.34)",
      "member_1 gave land to member_2 (value=-0.64)",
      "member_1 gave land to member_3 (value=-1.34)",
      "member_1 gave land to member_4 (value=-2.00)",
      "member_2 gave land to member_1 (value=-0.39)",
      "member_2 gave land to member_3 (value=2.10)",
      "member_2 gave land to member_4 (value=2.63)",
      "member_3 gave land to member_1 (value=1.36)",
      "member_3 gave land to member_2 (value=2.23)",
      "member_3 gave land to member_4 (value=1.30)",
      "member_4 gave land to member_1 (value=-1.38)",
      "member_4 gave land to member_2 (value=-2.79)",
      "member_4 gave land to member_3 (value=0.64)"
    ],
    "round_1_1": [
      "member_1 was attacked by member_2 (value=-37.18)",
      "member_1 was attacked by member_3 (value=-44.22)",
      "member_1 was attacked by member_4 (value=23.17)",
      "member_2 was attacked by member_1 (value=28.99)",
      "member_2 was attacked by member_3 (value=49.52)",
      "member_2 was attacked by member_4 (value=18.07)",
      "member_3 was attacked by member_1 (value=65.23)",
      "member_3 was attacked by member_2 (value=-38.21)",
      "member_3 was attacked by member_4 (value=9.15)",
      "member_4 was attacked by member_1 (value=-45.17)",
      "member_4 was attacked by member_2 (value=-8.52)",
      "member_4 was attacked by member_3 (value=-26.54)",
      "member_1 gave a benefit to member_2 (value=-26.13)",
      "member_1 gave a benefit to member_3 (value=-0.25)",
      "member_1 gave a benefit to member_4 (value=16.10)",
      "member_2 gave a benefit to member_1 (value=39.57)",
      "member_2 gave a benefit to member_3 (value=26.33)",
      "member_2 gave a benefit to member_4 (value=95.19)",
      "member_3 gave a benefit to member_1 (value=14.38)",
      "member_3 gave a benefit to member_2 (value=6.34)",
      "member_3 gave a benefit to member_4 (value=89.48)",
      "member_4 gave a benefit to member_1 (value=41.71)",
      "member_4 gave a benefit to member_2 (value=-0.75)",
      "member_4 gave a benefit to member_3 (value=67.34)",
      "member_1 gave land to member_2 (value=-0.64)",
      "member_1 gave land to member_3 (value=-1.34)",
      "member_1 gave land to member_4 (value=-2.00)",
      "member_2 gave land to member_1 (value=-0.39)",
      "member_2 gave land to member_3 (value=2.10)",
      "member_2 gave land to member_4 (value=2.63)",
      "member_3 gave land to member_1 (value=1.36)",
      "member_3 gave land to member_2 (value=2.23)",
      "member_3 gave land to member_4 (value=1.30)",
      "member_4 gave land to member_1 (value=-1.38)",
      "member_4 gave land to member_2 (value=-2.79)",
      "member_4 gave land to member_3 (value=0.64)"
    ],
    "round_1_2": [
      "member_1 was attacked by member_2 (value=-37.18)",
      "member_1 was attacked by member_3 (value=-44.22)",
      "member_1 was attacked by member_4 (value=23.17)",
      "member_2 was attacked by member_1 (value=28.99)",
      "member_2 was attacked by member_3 (value=49.52)",
      "member_2 was attacked by member_4 (value=18.07)",
      "member_3 was attacked by member_1 (value=65.23)",
      "member_3 was attacked by member_2 (value=-38.21)",
      "member_3 was attacked by member_4 (value=9.15)",
      "member_4 was attacked by member_1 (value=-45.17)",
      "member_4 was attacked by member_2 (value=-8.52)",
      "member_4 was attacked by member_3 (value=-26.54)",
      "member_1 gave a benefit to member_2 (value=-26.13)",
      "member_1 gave a benefit to member_3 (value=-0.25)",
      "member_1 gave a benefit to member_4 (value=16.10)",
      "member_2 gave a benefit to member_1 (value=39.57)",
      "member_2 gave a benefit to member_3 (value=26.33)",
      "member_2 gave a benefit to member_4 (value=95.19)",
      "member_3 gave a benefit to member_1 (value=14.38)",
      "member_3 gave a benefit to member_2 (value=6.34)",
      "member_3 gave a benefit to member_4 (value=89.48)",
      "member_4 gave a benefit to member_1 (value=41.71)",
      "member_4 gave a benefit to member_2 (value=-0.75)",
      "member_4 gave a benefit to member_3 (value=67.34)",
      "member_1 gave land to member_2 (value=-0.64)",
      "member_1 gave land to member_3 (value=-1.34)",
      "member_1 gave land to member_4 (value=-2.00)",
      "member_2 gave land to member_1 (value=-0.39)",
      "member_2 gave land to member_3 (value=2.10)",
      "member_2 gave land to member_4 (value=2.63)",
      "member_3 gave land to member_1 (value=1.36)",
      "member_3 gave land to member_2 (value=2.23)",
      "member_3 gave land to member_4 (value=1.30)",
      "member_4 gave land to member_1 (value=-1.38)",
      "member_4 gave land to member_2 (value=-2.79)",
      "member_4 gave land to member_3 (value=0.64)"
    ],
    "round_1_3": [
      "member_1 was attacked by member_2 (value=-37.18)",
      "member_1 was attacked by member_3 (value=-44.22)",
      "member_1 was attacked by member_4 (value=23.17)",
      "member_2 was attacked by member_1 (value=28.99)",
      "member_2 was attacked by member_3 (value=49.52)",
      "member_2 was attacked by member_4 (value=18.07)",
      "member_3 was attacked by member_1 (value=65.23)",
      "member_3 was attacked by member_2 (value=-38.21)",
      "member_3 was attacked by member_4 (value=9.15)",
      "member_4 was attacked by member_1 (value=-45.17)",
      "member_4 was attacked by member_2 (value=-8.52)",
      "member_4 was attacked by member_3 (value=-26.54)",
      "member_1 gave a benefit to member_2 (value=-26.13)",
      "member_1 gave a benefit to member_3 (value=-0.25)",
      "member_1 gave a benefit to member_4 (value=16.10)",
      "member_2 gave a benefit to member_1 (value=39.57)",
      "member_2 gave a benefit to member_3 (value=26.33)",
      "member_2 gave a benefit to member_4 (value=95.19)",
      "member_3 gave a benefit to member_1 (value=14.38)",
      "member_3 gave a benefit to member_2 (value=6.34)",
      "member_3 gave a benefit to member_4 (value=89.48)",
      "member_4 gave a benefit to member_1 (value=41.71)",
      "member_4 gave a benefit to member_2 (value=-0.75)",
      "member_4 gave a benefit to member_3 (value=67.34)",
      "member_1 gave land to member_2 (value=-0.64)",
      "member_1 gave land to member_3 (value=-1.34)",
      "member_1 gave land to member_4 (value=-2.00)",
      "member_2 gave land to member_1 (value=-0.39)",
      "member_2 gave land to member_3 (value=2.10)",
      "member_2 gave land to member_4 (value=2.63)",
      "member_3 gave land to member_1 (value=1.36)",
      "member_3 gave land to member_2 (value=2.23)",
      "member_3 gave land to member_4 (value=1.30)",
      "member_4 gave land to member_1 (value=-1.38)",
      "member_4 gave land to member_2 (value=-2.79)",
      "member_4 gave land to member_3 (value=0.64)"
    ]
  },
  "survival_tracking": {}
}